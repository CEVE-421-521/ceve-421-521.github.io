<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dr.&nbsp;James Doss-Gollin">

<title>CEVE 421/521 – Spring 2024 - Sequential Decision Problems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- MathJax 3 -->
<script>
window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="CEVE 421/521 – Spring 2024 - Sequential Decision Problems">
<meta property="og:description" content="Lecture">
<meta property="og:site_name" content="CEVE 421/521 -- Spring 2024">
</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#house-elevation-problem" id="toc-house-elevation-problem" class="nav-link" data-scroll-target="#house-elevation-problem">House elevation problem</a></li>
  <li><a href="#general-dynamic-decision-problems" id="toc-general-dynamic-decision-problems" class="nav-link" data-scroll-target="#general-dynamic-decision-problems">General dynamic decision problems</a></li>
  </ul></li>
  <li><a href="#optimal-control-and-reinforcement-learning" id="toc-optimal-control-and-reinforcement-learning" class="nav-link" data-scroll-target="#optimal-control-and-reinforcement-learning">Optimal control and reinforcement learning</a>
  <ul class="collapse">
  <li><a href="#framing" id="toc-framing" class="nav-link" data-scroll-target="#framing">Framing</a></li>
  <li><a href="#reward" id="toc-reward" class="nav-link" data-scroll-target="#reward">Reward</a></li>
  <li><a href="#policy" id="toc-policy" class="nav-link" data-scroll-target="#policy">Policy</a></li>
  <li><a href="#expected-future-rewards" id="toc-expected-future-rewards" class="nav-link" data-scroll-target="#expected-future-rewards">Expected future rewards</a></li>
  </ul></li>
  <li><a href="#solution-methods" id="toc-solution-methods" class="nav-link" data-scroll-target="#solution-methods">Solution methods</a>
  <ul class="collapse">
  <li><a href="#open-loop" id="toc-open-loop" class="nav-link" data-scroll-target="#open-loop">Open loop</a></li>
  <li><a href="#dynamic-programming" id="toc-dynamic-programming" class="nav-link" data-scroll-target="#dynamic-programming">Dynamic programming</a></li>
  <li><a href="#policy-search" id="toc-policy-search" class="nav-link" data-scroll-target="#policy-search">Policy search</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a>
  <ul class="collapse">
  <li><a href="#flexibility" id="toc-flexibility" class="nav-link" data-scroll-target="#flexibility">Flexibility</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://www.github.com/ceve-421-521/ceve-421-521.github.io/edit/main/lectures/09-sequential.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://www.github.com/ceve-421-521/ceve-421-521.github.io/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="09-sequential.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Sequential Decision Problems</h1>
<p class="subtitle lead">Lecture</p>
  <div class="quarto-categories">
    <div class="quarto-category">Lecture</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Dr.&nbsp;James Doss-Gollin </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">Mon., Mar.&nbsp;4</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>This lecture borrows heavily from <span class="citation" data-cites="herman_control:2020">Herman et al. (<a href="#ref-herman_control:2020" role="doc-biblioref">2020</a>)</span>.</p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<section id="house-elevation-problem" class="level2">
<h2 class="anchored" data-anchor-id="house-elevation-problem">House elevation problem</h2>
<p>Thus far, we have looked at a single <em>static</em> decision: how high to elevate the house. However, in many cases, decisions are not static, but rather <em>sequential</em>. For example, we don’t necessarily need to make this decision today! Instead, we could wait and see how fast local sea-levels are rising, and then make a decision later <strong>with more information</strong>.</p>
</section>
<section id="general-dynamic-decision-problems" class="level2">
<h2 class="anchored" data-anchor-id="general-dynamic-decision-problems">General dynamic decision problems</h2>
<p>Dynamic planning problems identify policies to select actions in response <em>to new information over time</em>. Policy design involves choosing the sequence, timing, and/or threshold of actions to achieve a desired outcome. This typically involves a combination of optimal control and adaptive design.</p>
</section>
</section>
<section id="optimal-control-and-reinforcement-learning" class="level1">
<h1>Optimal control and reinforcement learning</h1>
<p>Optimal control and reinforcement learning are related fields relating to the study of optimizing sequential decision problems.</p>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>For a thorough but accessible textbook on reinforcement learning, I recommend <span class="citation" data-cites="sutton_reinforcement:2018">Sutton &amp; Barto (<a href="#ref-sutton_reinforcement:2018" role="doc-biblioref">2018</a>)</span>.</p>
</div>
</div>
</div>
<section id="framing" class="level2">
<h2 class="anchored" data-anchor-id="framing">Framing</h2>
<p>In sequential decision problems, the decision maker does not need to make all decisions at once. Instead, at each time step, the decision maker makes a decision based on the <strong>state</strong> of the system (which may not be fully observable). In our case study, the state might include the current elevation of the house, the current sea level, and other potential variables.</p>
<p>Mathematically, the state evolves over time according to a dynamics model, which describes how the state changes in response to the decision maker’s actions and external factors: <span class="math display">\[
\mathbf{x}_{t+1} = f_t(\mathbf{x}_t, a_t, e_{t+1}),
\]</span> where</p>
<ul>
<li><span class="math inline">\(\mathbf{x}_t\)</span> is the state at time <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(a_t\)</span> is the decision at time <span class="math inline">\(t\)</span> (<em>e.g.&nbsp;whether we elevate and if so how high</em>)</li>
<li><span class="math inline">\(e_{t+1}\)</span> is some forcing (<em>e.g., the rate of sea level rise</em>)</li>
<li><span class="math inline">\(f_t\)</span> is assumed deterministic, but it can evolve over time.</li>
</ul>
</section>
<section id="reward" class="level2">
<h2 class="anchored" data-anchor-id="reward">Reward</h2>
<p>A key concept in sequential decision problems is that at each time step, the decision maker gets some immediate feedback. This is often called reward, <span class="math inline">\(R\)</span>. The reward <span class="math inline">\(R_{t+1}\)</span> (indices by convention) depends on the state at time <span class="math inline">\(t\)</span>, the action at time <span class="math inline">\(t\)</span>, and the forcing at time <span class="math inline">\(t\)</span>.</p>
<p>In our house elevation problem, the reward might be the cost of flood insurance and the cost of elevating (which will often be zero).</p>
</section>
<section id="policy" class="level2">
<h2 class="anchored" data-anchor-id="policy">Policy</h2>
<p>The decision maker’s strategy for choosing actions is called a <strong>policy</strong>. The policy is a deterministic (or stochastic) function that maps states to actions. We’ll focus here on <em>discrete time</em> problems, although <em>continuous time</em> problems are also well-studied in some domains.</p>
<div id="fig-rl" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../_assets/img/rl-sketch.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A sketch of the reinforcement learning problem.
</figcaption>
</figure>
</div>
</section>
<section id="expected-future-rewards" class="level2">
<h2 class="anchored" data-anchor-id="expected-future-rewards">Expected future rewards</h2>
<p>A central idea of optimal control problems is to maximize the expected sum of future rewards.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> The basic idea is that there might be actions that give a low reward now, but that lead to high rewards in the future. In our case study, an illustration would be spending a lot of money to elevate the house now, but then not having to pay as much for flood insurance in the future. Future rewards are usually discounted (as we have seen); this can be done either in the cost function or in the reward function.</p>
<p>This leads naturally to an important concept in reinforcement learning: <strong>value</strong>. The value of a state is the expected sum of future rewards that can be obtained from that state, assuming the decision maker follows a particular policy. The value of a state-action pair is the expected sum of future rewards that can be obtained from that state, assuming the decision maker takes a particular action and then follows a particular policy.</p>
</section>
</section>
<section id="solution-methods" class="level1">
<h1>Solution methods</h1>
<p>There are many methods for solving sequential decision problems! Here, we’ll focus on a few examples from <span class="citation" data-cites="herman_control:2020">Herman et al. (<a href="#ref-herman_control:2020" role="doc-biblioref">2020</a>)</span>. This is not by any means a comprehensive list.</p>
<p>On Wednesday we’ll read a paper that compares what we call here open loop (they call it “intertemporal open loop control”) and dynamic policy search.</p>
<section id="open-loop" class="level2">
<h2 class="anchored" data-anchor-id="open-loop">Open loop</h2>
<p>Open loop control solves for all actions at once. The result is a vector of actions corresponding to each time step.</p>
<p>The primary advantage of open loop control is that it’s very easy to execute the policy – no further analysis, updating, or optimization is needed. The primary disadvantage is that it’s not adaptive – it doesn’t take into account new information that might be available later. It can also be computationally challenging because it requires solving a lot of decision variables (each time step is a decision variable) and they are not independent (if I elevate my house in 2030, I probably don’t want to elevate it in 2031).</p>
</section>
<section id="dynamic-programming" class="level2">
<h2 class="anchored" data-anchor-id="dynamic-programming">Dynamic programming</h2>
<p>There are many variations of dynamic programming, but the most commonly applied is stochastic dynamic programming, in which the value function <span class="math inline">\(Q\)</span> for each state at time <span class="math inline">\(t\)</span> is estimated from the recursive Bellman equation: <span class="math display">\[
Q_t(\mathbf{x}_t) = \min_{a_t} \left\{ R_t + \gamma Q_{t+1} (\mathbf{x}_{t+1}) \right\}.
\]</span> where <span class="math inline">\(\gamma\)</span> is the discount factor.</p>
<p>This problem is typically discretized and solved using a backward induction algorithm. In other words, a discrete number of states and actions are considered. The discrete state transition function gives you the probability of transitioning from one state to another, given an action. A very rough solution approach is:</p>
<ol type="1">
<li>Calculate the value function for the each step in last time step, <span class="math inline">\(Q_T(x^1_T), Q_T(x^2_T), \ldots\)</span>.</li>
<li>For each time step <span class="math inline">\(t = T-1, T-2, \ldots, 1\)</span>, calculate the value function for each state <span class="math inline">\(x_t\)</span> using the Bellman equation:
<ol type="1">
<li>For each state <span class="math inline">\(x_t\)</span>, calculate the value of each action <span class="math inline">\(a_t\)</span> as the reward plus the discounted <em>expected value</em> of the next state</li>
<li>Choose the action that maximizes the value function</li>
<li>The value function for the state is the value of the action that maximizes the value function</li>
</ol></li>
</ol>
<p>An advantage of methods like SDP is that they can provide exact solutions to the problem, conditional on the model and discretization. A disadvantage is that often very strong assumptions are required to discretize the problem and make it tractable. SDP also suffers from the “curse of dimensionality” because as the number of states and actions increases, the number of possible state-action pairs increases exponentially.</p>
</section>
<section id="policy-search" class="level2">
<h2 class="anchored" data-anchor-id="policy-search">Policy search</h2>
<p>Policy search assumes a specific functional form for a policy <span class="math inline">\(\pi\)</span> with parameters <span class="math inline">\(\theta\)</span> such that <span class="math inline">\(a_t = f(\mathbf{x}_t, \theta)\)</span>. The goal is to find the parameters <span class="math inline">\(\theta\)</span> that maximize the expected sum of future rewards.</p>
<p>Common choices of <span class="math inline">\(f\)</span> include linear decision rules, radial basis functions, binary trees, and neural networks. A primary advantage of policy search is that it can be very flexible and adaptive, and can be used with <strong>simulation-optimization</strong> frameworks. A primary disadvantage is that it can be computationally expensive and can require a lot of data to estimate the parameters.</p>
<p>In our case study, there are many different ways to parameterize our decision rule. Two options suggested by <span class="citation" data-cites="garnergarner_slrise:2018">(<a href="#ref-garnergarner_slrise:2018" role="doc-biblioref"><strong>garnergarner_slrise:2018?</strong></a>)</span> are:</p>
<ol type="1">
<li>define a “buffer height” as the minimum tolerable elevation of the house relative to the mean sea level and a “freeboard height” so that when the buffer height is exceeded, the house is elevated to be BH + FH above the mean sea level.</li>
<li>Estimate FH and BH based on the local slope and acceleration of the sea-level</li>
</ol>
<p>However, very complex control rules using deep neural networks are also used (e.g., in video game playing).</p>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p>Like everything we’ve seen in this class, we are using a model to represent the real world, but the model isn’t the real world and we should take this into account when designing and interpreting our optimization.</p>
<section id="flexibility" class="level2">
<h2 class="anchored" data-anchor-id="flexibility">Flexibility</h2>
<p>The financial theory of real options defines “the right, but not obligation” to take a particular action in the future. For example, if I purchase the right to buy Apple stocks in 1 year at price <span class="math inline">\(X\)</span>, I can choose to buy the stocks if the price is higher than <span class="math inline">\(X\)</span>, but I don’t have to buy them if the price is lower than <span class="math inline">\(X\)</span>. This is a real option, and it has value.</p>
<p>This has motivated study of options in engineering and policy design. It turns out that creating flexibility in decision making can be very valuable, but it’s not always easy to generate designs that are naturally flexible. An example we’ll see on Friday gives a simple example: build a parking garage to a few floors, build it to many floors, or build a few floors but extra-strong so we can add levels in the future if needed? <span class="citation" data-cites="deneufville_parkinggarage:2006">(<a href="#ref-deneufville_parkinggarage:2006" role="doc-biblioref">Neufville et al., 2006</a>)</span>.</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-herman_control:2020" class="csl-entry" role="listitem">
Herman, J. D., Quinn, J. D., Steinschneider, S., Giuliani, M., &amp; Fletcher, S. (2020). Climate adaptation as a control problem: <span>Review</span> and perspectives on dynamic water resources planning under uncertainty. <em>Water Resources Research</em>, e24389. <a href="https://doi.org/10.1029/2019wr025502">https://doi.org/10.1029/2019wr025502</a>
</div>
<div id="ref-deneufville_parkinggarage:2006" class="csl-entry" role="listitem">
Neufville, R. de, Scholtes, S., &amp; Wang, T. (2006). Real <span>Options</span> by <span>Spreadsheet</span>: <span>Parking Garage Case Example</span>. <em>Journal of Infrastructure Systems</em>, <em>12</em>(2), 107–111. <a href="https://doi.org/10.1061/(asce)1076-0342(2006)12:2(107)">https://doi.org/10.1061/(asce)1076-0342(2006)12:2(107)</a>
</div>
<div id="ref-sutton_reinforcement:2018" class="csl-entry" role="listitem">
Sutton, R. S., &amp; Barto, A. G. (2018). <em>Reinforcement learning: An <span>Introduction</span></em> (Second Edition). Cambridge, Massachusetts; London, England: MIT Press.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>This is a slight simplification; equation 1 of <span class="citation" data-cites="herman_control:2020">Herman et al. (<a href="#ref-herman_control:2020" role="doc-biblioref">2020</a>)</span> gives a more general form.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Content <i class="fa-solid fa-copyright" aria-label="copyright"></i> 2024 by <a href="https://jamesdossgollin.me">James Doss-Gollin</a>. All content licensed under a <i class="fa-brands fa-creative-commons" aria-label="creative-commons"></i> <i class="fa-brands fa-creative-commons-by" aria-label="creative-commons-by"></i> <i class="fa-brands fa-creative-commons-nc" aria-label="creative-commons-nc"></i> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license (CC BY-NC-SA 4.0)</a></p>
</div>   
    <div class="nav-footer-center">
<p><strong>Page still under construction.</strong></p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://www.github.com/ceve-421-521/ceve-421-521.github.io/edit/main/lectures/09-sequential.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://www.github.com/ceve-421-521/ceve-421-521.github.io/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Made with <a href="https://julialang.org">Julia</a> and <a href="https://quarto.org/">Quarto</a><br> <a href="https://www.github.com/ceve543/ceve543.github.io">View the source on <i class="fa-brands fa-github" aria-label="github"></i> GitHub</a></p>
</div>
  </div>
</footer>




</body></html>