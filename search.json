[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "CEVE 421/521 Schedule",
    "section": "",
    "text": "Reading\n\n\n\nA subset of readings listed here will likely be assigned. Check the page for each reading to see discussion questions.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Topic\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nWeek\n\n\nTopic\n\n\nDate\n\n\nCategory\n\n\n\n\n\n\n1\n\n\nWelcome to CEVE 421/521!\n\n\nMon., Jan. 8\n\n\nLecture\n\n\n\n\n1\n\n\nWeek 1 readings\n\n\nWed., Jan. 10\n\n\nReading\n\n\n\n\n1\n\n\nLab 1: Software Installation\n\n\nFri., Jan. 12\n\n\nLab\n\n\n\n\n2\n\n\nThe Science of Climate Hazard\n\n\nWed., Jan. 17\n\n\nLecture\n\n\n\n\n2\n\n\nWeek 2 readings\n\n\nWed., Jan. 17\n\n\nReading\n\n\n\n\n2\n\n\nLab 2: Julia Quickstart\n\n\nFri., Jan. 19\n\n\nLab\n\n\n\n\n3\n\n\nVulnerability, Exposure, and Impacts\n\n\nMon., Jan. 22\n\n\nLecture\n\n\n\n\n3\n\n\nReadings for Week 3\n\n\nWed., Jan. 24\n\n\nReading\n\n\n\n\n3\n\n\nLab 3: Depth-Damage Models\n\n\nFri., Jan. 26\n\n\nLab\n\n\n\n\n4\n\n\nClimate Risks to Complex Sytems\n\n\nMon., Jan. 29\n\n\nLecture\n\n\n\n\n4\n\n\nExam 1\n\n\nFri., Feb. 2\n\n\nExam\n\n\n\n\n5\n\n\nCost-Benefit Analysis\n\n\nMon., Feb. 5\n\n\nLecture\n\n\n\n\n5\n\n\nCost-Benefit Analysis II\n\n\nWed., Feb. 7\n\n\nLecture\n\n\n\n\n5\n\n\nLab 4: House Elevation NPV Analysis\n\n\nThu., Feb. 8\n\n\nLab\n\n\n\n\n6\n\n\nScenario Analysis\n\n\nMon., Feb. 12\n\n\nLecture\n\n\n\n\n6\n\n\nReadings for Week 6\n\n\nWed., Feb. 14\n\n\nReading\n\n\n\n\n6\n\n\nLab 5: Sea-Level Rise\n\n\nFri., Feb. 16\n\n\nLab\n\n\n\n\n7\n\n\nPolicy Search and Optimization\n\n\nMon., Feb. 19\n\n\nLecture\n\n\n\n\n7\n\n\nReadings for Week 7\n\n\nWed., Feb. 21\n\n\nReading\n\n\n\n\n8\n\n\nMultiobjective Policy Search and Optimization\n\n\nMon., Feb. 26\n\n\nLecture\n\n\n\n\n8\n\n\nReadings for Week 8\n\n\nWed., Feb. 28\n\n\nReading\n\n\n\n\n7\n\n\nLab 6: Policy Search\n\n\nFri., Mar. 1\n\n\nLab\n\n\n\n\n9\n\n\nSequential Decision Problems\n\n\nMon., Mar. 4\n\n\nLecture\n\n\n\n\n9\n\n\nReadings for Week 9\n\n\nWed., Mar. 6\n\n\nReading\n\n\n\n\n9\n\n\nLab 7: Parking Garage Case Study\n\n\nFri., Mar. 8\n\n\nLab\n\n\n\n\n10\n\n\nRobustness\n\n\nMon., Mar. 18\n\n\nLecture\n\n\n\n\n10\n\n\nReadings for Week 10\n\n\nWed., Mar. 20\n\n\nReading\n\n\n\n\n10\n\n\nExam 2 Review\n\n\nFri., Mar. 22\n\n\nExam\n\n\n\n\n11\n\n\nDeep Uncertainty\n\n\nMon., Mar. 25\n\n\nLecture\n\n\n\n\n11\n\n\nReadings for Week 11\n\n\nWed., Mar. 27\n\n\nReading\n\n\n\n\n11\n\n\nExam 2\n\n\nFri., Mar. 29\n\n\nExam\n\n\n\n\n12\n\n\nEquity and Justice\n\n\nMon., Apr. 1\n\n\nLecture\n\n\n\n\n12\n\n\nReadings for Week 12\n\n\nWed., Apr. 3\n\n\nReading\n\n\n\n\n10\n\n\nLab 8: The Lake Problem\n\n\nFri., Apr. 5\n\n\nLab\n\n\n\n\n13\n\n\nFinancial and Systemic Risks\n\n\nWed., Apr. 10\n\n\nLecture\n\n\n\n\n13\n\n\nReadings for Week 13\n\n\nWed., Apr. 10\n\n\nReading\n\n\n\n\n14\n\n\nReflections\n\n\nMon., Apr. 15\n\n\nLecture\n\n\n\n\n14\n\n\nReadings for Week 14\n\n\nWed., Apr. 17\n\n\nReading\n\n\n\n\n14\n\n\nExam 3\n\n\nFri., Apr. 19\n\n\nExam\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "lectures/06-scenario-analysis.html",
    "href": "lectures/06-scenario-analysis.html",
    "title": "Scenario Analysis",
    "section": "",
    "text": "Let’s tighten our notation just a little bit\n\n\n\n\n\n\nFigure 1: Notation for our system dynamics.\n\n\n\n\nState of the world: encapsulates all inputs to our model.\nDecisions: can be very simple (how high do we elevate a house right now?) or very complex (spatial and/or temporal optimization problems)\nOutcomes: can be a single number (scalar) or a vector if there are multiple outcomes we care about.\n\n\n\n\n\nDo this for a single state of the world\nEmphasis on discounted cash flow to define “outcomes”\n\n\n\n\nUncertainty commonly divided into two broad classes:\n\nAleatory uncertainty, or uncertainties resulting from randomness;\nEpistemic uncertainty, or uncertainties resulting from lack of knowledge.\n\nWe can also categorize uncertainty based on the source of the uncertainty:\n\nParameter uncertainty\nModel structure uncertainty\nExternal / boundary condition / scenario uncertainty\n\nUncertainty is represented in models in many different ways. For example:\n\nDeterministic: hold some things fixed (we always do this!)\nProbabilistic with expectations (lab)\nProbabilistic with sampling\n\nThese are all assumptions, and these assumptions can be varied."
  },
  {
    "objectID": "lectures/06-scenario-analysis.html#notation",
    "href": "lectures/06-scenario-analysis.html#notation",
    "title": "Scenario Analysis",
    "section": "",
    "text": "Let’s tighten our notation just a little bit\n\n\n\n\n\n\nFigure 1: Notation for our system dynamics.\n\n\n\n\nState of the world: encapsulates all inputs to our model.\nDecisions: can be very simple (how high do we elevate a house right now?) or very complex (spatial and/or temporal optimization problems)\nOutcomes: can be a single number (scalar) or a vector if there are multiple outcomes we care about."
  },
  {
    "objectID": "lectures/06-scenario-analysis.html#cost-benefit-analysis",
    "href": "lectures/06-scenario-analysis.html#cost-benefit-analysis",
    "title": "Scenario Analysis",
    "section": "",
    "text": "Do this for a single state of the world\nEmphasis on discounted cash flow to define “outcomes”"
  },
  {
    "objectID": "lectures/06-scenario-analysis.html#uncertainty",
    "href": "lectures/06-scenario-analysis.html#uncertainty",
    "title": "Scenario Analysis",
    "section": "",
    "text": "Uncertainty commonly divided into two broad classes:\n\nAleatory uncertainty, or uncertainties resulting from randomness;\nEpistemic uncertainty, or uncertainties resulting from lack of knowledge.\n\nWe can also categorize uncertainty based on the source of the uncertainty:\n\nParameter uncertainty\nModel structure uncertainty\nExternal / boundary condition / scenario uncertainty\n\nUncertainty is represented in models in many different ways. For example:\n\nDeterministic: hold some things fixed (we always do this!)\nProbabilistic with expectations (lab)\nProbabilistic with sampling\n\nThese are all assumptions, and these assumptions can be varied."
  },
  {
    "objectID": "lectures/06-scenario-analysis.html#examples",
    "href": "lectures/06-scenario-analysis.html#examples",
    "title": "Scenario Analysis",
    "section": "Examples",
    "text": "Examples\nLet’s return to our house elevation problem. What are some things we could consider?\n\nSea-level rise: could consider a few different scenarios of sea-level rise (e.g., Sweet et al., 2022)\nDiscount rate"
  },
  {
    "objectID": "lectures/06-scenario-analysis.html#experiment-design",
    "href": "lectures/06-scenario-analysis.html#experiment-design",
    "title": "Scenario Analysis",
    "section": "Experiment design",
    "text": "Experiment design\n\nNumber of scenarios\n\nA few = interpretable\nMany = more systematic analysis. Interactions between different sources of uncertainty are important.\n\nWhat is a scenario?\n\nParameter values\nModel structure\n\nHow to generate / sample scenarios"
  },
  {
    "objectID": "lectures/08-multiobjective.html#references",
    "href": "lectures/08-multiobjective.html#references",
    "title": "Multiobjective Policy Search and Optimization",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nKasprzyk, J. R., Nataraj, S., Reed, P. M., & Lempert, R. J. (2013). Many objective robust decision making for complex environmental systems undergoing change. Environmental Modelling & Software, 42, 55–71. https://doi.org/10.1016/j.envsoft.2012.12.007\n\n\nSmith, S., Southerby, M., Setiniyaz, S., Apsimon, R., & Burt, G. (2022). Multiobjective optimization and Pareto front visualization techniques applied to normal conducting rf accelerating structures. Physical Review Accelerators and Beams, 25(6), 062002. https://doi.org/10.1103/PhysRevAccelBeams.25.062002\n\n\nZarekarizi, M., Srikrishnan, V., & Keller, K. (2020). Neglecting uncertainties biases house-elevation decisions to manage riverine flood risks. Nature Communications, 11(1, 1), 5361. https://doi.org/10.1038/s41467-020-19188-9"
  },
  {
    "objectID": "lectures/10-robustness.html#references",
    "href": "lectures/10-robustness.html#references",
    "title": "Robustness",
    "section": "References",
    "text": "References\n\n\nAbraham, S., Diringer, S., & Cooley, H. (2020). An Assessment of Urban Water Demand Forecasts in California. Oakland, California: Pacific Institute. Retrieved from https://pacinst.org/wp-content/uploads/2020/08/Pacific-Institute-Assessment-Urban-Water-Demand-Forecasts-in-CA-Aug-2020.pdf\n\n\nDoss-Gollin, J., & Keller, K. (2023). A subjective Bayesian framework for synthesizing deep uncertainties in climate risk management. Earth’s Future, 11(1). https://doi.org/10.1029/2022EF003044\n\n\nHausfather, Z., & Peters, G. P. (2020). Emissions – the ’business as usual’ story is misleading. Nature, 577(7792, 7792), 618–620. https://doi.org/10.1038/d41586-020-00177-3\n\n\nHerman, J. D., Reed, P. M., Zeff, H. B., & Characklis, G. W. (2015). How should robustness be defined for water systems planning under change? Journal of Water Resources Planning and Management, 141(10), 04015012. https://doi.org/10.1061/(asce)wr.1943-5452.0000509\n\n\nMatalas, N. C., & Fiering, M. B. (1977). 6. Water-Resource Systems Planning. In Climate, Climatic Change, and Water Supply (pp. 99–110). Washington, DC: The National Academies Press. Retrieved from https://www.nap.edu/read/185/chapter/11\n\n\nMcPhail, C., Maier, H. R., Kwakkel, J. H., Giuliani, M., Castelletti, A., & Westra, S. (2019). Robustness metrics: How are they calculated, when should they be used and why do they give different results? Earth’s Future, 169–191. https://doi.org/10.1002/2017ef000649"
  },
  {
    "objectID": "lectures/09-sequential.html",
    "href": "lectures/09-sequential.html",
    "title": "Sequential Decision Problems",
    "section": "",
    "text": "This lecture borrows heavily from Herman et al. (2020)."
  },
  {
    "objectID": "lectures/09-sequential.html#house-elevation-problem",
    "href": "lectures/09-sequential.html#house-elevation-problem",
    "title": "Sequential Decision Problems",
    "section": "House elevation problem",
    "text": "House elevation problem\nThus far, we have looked at a single static decision: how high to elevate the house. However, in many cases, decisions are not static, but rather sequential. For example, we don’t necessarily need to make this decision today! Instead, we could wait and see how fast local sea-levels are rising, and then make a decision later with more information."
  },
  {
    "objectID": "lectures/09-sequential.html#general-dynamic-decision-problems",
    "href": "lectures/09-sequential.html#general-dynamic-decision-problems",
    "title": "Sequential Decision Problems",
    "section": "General dynamic decision problems",
    "text": "General dynamic decision problems\nDynamic planning problems identify policies to select actions in response to new information over time. Policy design involves choosing the sequence, timing, and/or threshold of actions to achieve a desired outcome. This typically involves a combination of optimal control and adaptive design."
  },
  {
    "objectID": "lectures/09-sequential.html#framing",
    "href": "lectures/09-sequential.html#framing",
    "title": "Sequential Decision Problems",
    "section": "Framing",
    "text": "Framing\nIn sequential decision problems, the decision maker does not need to make all decisions at once. Instead, at each time step, the decision maker makes a decision based on the state of the system (which may not be fully observable). In our case study, the state might include the current elevation of the house, the current sea level, and other potential variables.\nMathematically, the state evolves over time according to a dynamics model, which describes how the state changes in response to the decision maker’s actions and external factors: \\[\n\\mathbf{x}_{t+1} = f_t(\\mathbf{x}_t, a_t, e_{t+1}),\n\\] where\n\n\\(\\mathbf{x}_t\\) is the state at time \\(t\\)\n\\(a_t\\) is the decision at time \\(t\\) (e.g. whether we elevate and if so how high)\n\\(e_{t+1}\\) is some forcing (e.g., the rate of sea level rise)\n\\(f_t\\) is assumed deterministic, but it can evolve over time."
  },
  {
    "objectID": "lectures/09-sequential.html#reward",
    "href": "lectures/09-sequential.html#reward",
    "title": "Sequential Decision Problems",
    "section": "Reward",
    "text": "Reward\nA key concept in sequential decision problems is that at each time step, the decision maker gets some immediate feedback. This is often called reward, \\(R\\). The reward \\(R_{t+1}\\) (indices by convention) depends on the state at time \\(t\\), the action at time \\(t\\), and the forcing at time \\(t\\).\nIn our house elevation problem, the reward might be the cost of flood insurance and the cost of elevating (which will often be zero)."
  },
  {
    "objectID": "lectures/09-sequential.html#policy",
    "href": "lectures/09-sequential.html#policy",
    "title": "Sequential Decision Problems",
    "section": "Policy",
    "text": "Policy\nThe decision maker’s strategy for choosing actions is called a policy. The policy is a deterministic (or stochastic) function that maps states to actions. We’ll focus here on discrete time problems, although continuous time problems are also well-studied in some domains.\n\n\n\n\n\n\nFigure 1: A sketch of the reinforcement learning problem."
  },
  {
    "objectID": "lectures/09-sequential.html#expected-future-rewards",
    "href": "lectures/09-sequential.html#expected-future-rewards",
    "title": "Sequential Decision Problems",
    "section": "Expected future rewards",
    "text": "Expected future rewards\nA central idea of optimal control problems is to maximize the expected sum of future rewards.1 The basic idea is that there might be actions that give a low reward now, but that lead to high rewards in the future. In our case study, an illustration would be spending a lot of money to elevate the house now, but then not having to pay as much for flood insurance in the future. Future rewards are usually discounted (as we have seen); this can be done either in the cost function or in the reward function.\nThis leads naturally to an important concept in reinforcement learning: value. The value of a state is the expected sum of future rewards that can be obtained from that state, assuming the decision maker follows a particular policy. The value of a state-action pair is the expected sum of future rewards that can be obtained from that state, assuming the decision maker takes a particular action and then follows a particular policy."
  },
  {
    "objectID": "lectures/09-sequential.html#open-loop",
    "href": "lectures/09-sequential.html#open-loop",
    "title": "Sequential Decision Problems",
    "section": "Open loop",
    "text": "Open loop\nOpen loop control solves for all actions at once. The result is a vector of actions corresponding to each time step.\nThe primary advantage of open loop control is that it’s very easy to execute the policy – no further analysis, updating, or optimization is needed. The primary disadvantage is that it’s not adaptive – it doesn’t take into account new information that might be available later. It can also be computationally challenging because it requires solving a lot of decision variables (each time step is a decision variable) and they are not independent (if I elevate my house in 2030, I probably don’t want to elevate it in 2031)."
  },
  {
    "objectID": "lectures/09-sequential.html#dynamic-programming",
    "href": "lectures/09-sequential.html#dynamic-programming",
    "title": "Sequential Decision Problems",
    "section": "Dynamic programming",
    "text": "Dynamic programming\nThere are many variations of dynamic programming, but the most commonly applied is stochastic dynamic programming, in which the value function \\(Q\\) for each state at time \\(t\\) is estimated from the recursive Bellman equation: \\[\nQ_t(\\mathbf{x}_t) = \\min_{a_t} \\left\\{ R_t + \\gamma Q_{t+1} (\\mathbf{x}_{t+1}) \\right\\}.\n\\] where \\(\\gamma\\) is the discount factor.\nThis problem is typically discretized and solved using a backward induction algorithm. In other words, a discrete number of states and actions are considered. The discrete state transition function gives you the probability of transitioning from one state to another, given an action. A very rough solution approach is:\n\nCalculate the value function for the each step in last time step, \\(Q_T(x^1_T), Q_T(x^2_T), \\ldots\\).\nFor each time step \\(t = T-1, T-2, \\ldots, 1\\), calculate the value function for each state \\(x_t\\) using the Bellman equation:\n\nFor each state \\(x_t\\), calculate the value of each action \\(a_t\\) as the reward plus the discounted expected value of the next state\nChoose the action that maximizes the value function\nThe value function for the state is the value of the action that maximizes the value function\n\n\nAn advantage of methods like SDP is that they can provide exact solutions to the problem, conditional on the model and discretization. A disadvantage is that often very strong assumptions are required to discretize the problem and make it tractable. SDP also suffers from the “curse of dimensionality” because as the number of states and actions increases, the number of possible state-action pairs increases exponentially."
  },
  {
    "objectID": "lectures/09-sequential.html#policy-search",
    "href": "lectures/09-sequential.html#policy-search",
    "title": "Sequential Decision Problems",
    "section": "Policy search",
    "text": "Policy search\nPolicy search assumes a specific functional form for a policy \\(\\pi\\) with parameters \\(\\theta\\) such that \\(a_t = f(\\mathbf{x}_t, \\theta)\\). The goal is to find the parameters \\(\\theta\\) that maximize the expected sum of future rewards.\nCommon choices of \\(f\\) include linear decision rules, radial basis functions, binary trees, and neural networks. A primary advantage of policy search is that it can be very flexible and adaptive, and can be used with simulation-optimization frameworks. A primary disadvantage is that it can be computationally expensive and can require a lot of data to estimate the parameters.\nIn our case study, there are many different ways to parameterize our decision rule. Two options suggested by (garnergarner_slrise:2018?) are:\n\ndefine a “buffer height” as the minimum tolerable elevation of the house relative to the mean sea level and a “freeboard height” so that when the buffer height is exceeded, the house is elevated to be BH + FH above the mean sea level.\nEstimate FH and BH based on the local slope and acceleration of the sea-level\n\nHowever, very complex control rules using deep neural networks are also used (e.g., in video game playing)."
  },
  {
    "objectID": "lectures/09-sequential.html#flexibility",
    "href": "lectures/09-sequential.html#flexibility",
    "title": "Sequential Decision Problems",
    "section": "Flexibility",
    "text": "Flexibility\nThe financial theory of real options defines “the right, but not obligation” to take a particular action in the future. For example, if I purchase the right to buy Apple stocks in 1 year at price \\(X\\), I can choose to buy the stocks if the price is higher than \\(X\\), but I don’t have to buy them if the price is lower than \\(X\\). This is a real option, and it has value.\nThis has motivated study of options in engineering and policy design. It turns out that creating flexibility in decision making can be very valuable, but it’s not always easy to generate designs that are naturally flexible. An example we’ll see on Friday gives a simple example: build a parking garage to a few floors, build it to many floors, or build a few floors but extra-strong so we can add levels in the future if needed? (Neufville et al., 2006)."
  },
  {
    "objectID": "lectures/09-sequential.html#footnotes",
    "href": "lectures/09-sequential.html#footnotes",
    "title": "Sequential Decision Problems",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is a slight simplification; equation 1 of Herman et al. (2020) gives a more general form.↩︎"
  },
  {
    "objectID": "lectures/05-bca2.html",
    "href": "lectures/05-bca2.html",
    "title": "Cost-Benefit Analysis II",
    "section": "",
    "text": "Here’s a brief recap, with a few additional points.\n\nDefining “goodness” of different outcomes\n\nWe need a utility / objective / loss function to compare different outcomes: \\(u(a, \\mathbf{s}): \\mathcal{A} \\times \\mathcal{S} \\to \\mathbb{R}\\) where \\(\\mathcal{A}\\) is the set of actions and \\(\\mathcal{S}\\) is the set of states of the world.\nMathematically and practically, we can define “goodness” however we want! We are free to choose our metric of goodness to compare different decisions.\nUtility: “rational” people make decisions as though maximizing some internal, invisible utility function. This leads to useful insights from the field of economics.\n\nKey insight: decreasing marginal utility. When you have a lot of most things, getting a little bit more is not as valuable as when you have very little of it. For example, $1,000 is worth a lot more to a poor person than to a rich person. 100 gallons of water are more valuable to someone who walks 5 miles a day carrying water than to someone who has a tap in their kitchen.\nWe should be skeptical of utility as a framework for predicting human behavior, but that’s not necessarily our goal.\n\nSocial welfare: even if you believe everyone has a utility function, it turns out that there’s not a a single “right” way to combine many peoples’ utility functions into a single number describing social welfare.\nCash flow: often, we put things into dollar terms. This is not because we think money is the most important thing. Instead, we can look at opportunity costs to try to put dollar values on things that are hard to value. For example, if we can protect one square mile of pristine wetland for $1 million, then arguably we shouldn’t spend more than that to protect another square mile of wetland. (But are they really substitutes?)\nDiscounted cash flow: a dollar today is worth more than a dollar tomorrow. To compute the net present value (NPV) of a future cost or benefit, we discount it to the present using a discount rate \\(\\gamma\\). If we define \\(\\gamma = 2\\%\\) then a dollar in 10 years is worth \\((1 - \\gamma)^{10} =  \\$0.82\\) today.\n\nCost-benefit analysis under uncertainty\n\nMany analyses in the real world choose a single “best guess” or “representative” state of the world (often referred to as scenario or similar). For example, for analyzing the costs and benefits of expanding a water resource system, a hypothetical water utility in Houston might use:\n\nprojections of future population / demand from a regional water plan\na single projected climate scenario\na single assumed set of regulations on water quality\ncurrent prices of building / operating water infrastructure\n\nHowever, many decisions that perform well under one scenario perform poorly under another. For example, a water utility might choose to build a new reservoir to meet future demand, but if the future is drier than expected, the reservoir might not fill up. Or, if the future is wetter than expected, the reservoir might fill up too much and flood people’s homes. (This will be a theme of the course!)\nA better approach is to allow uncertainty by computing expected utility / objective / loss / metrics. This is a weighted average of the utility under different states of the world, where the weights are the probabilities of those states of the world.\n\n\\(\\mathbb{E}_\\mathbf{s} \\left[ u(a, \\mathbf{s}) \\right] = \\int p(\\mathbf{s}) u(a, \\mathbf{s}) d\\mathbf{s}\\) where \\(p(\\mathbf{s})\\) is the probability density of SOW \\(\\mathbf{s}\\).\nThis requires defining a probability distribution over states of the world."
  },
  {
    "objectID": "lectures/05-bca2.html#proposed-solutions",
    "href": "lectures/05-bca2.html#proposed-solutions",
    "title": "Cost-Benefit Analysis II",
    "section": "Proposed solutions",
    "text": "Proposed solutions\n\nUse a very low discount rate\n\nThis places more weight on outcomes far in the future\nHowever, this can also lead to irrational conclusions. For example,\n\nUncertain discount rate\n\n“Using an investment- or a consumption-based approach will require human judgment about the appropriate models to use to capture uncertainty about future discount rates” (Arrow et al., 2013)\nThis is equivalent to declining discount rate – as you go farther out, your NPV is dominated by the lower discount rates.\nYou still need to"
  },
  {
    "objectID": "lectures/07-policy-search.html#references",
    "href": "lectures/07-policy-search.html#references",
    "title": "Policy Search and Optimization",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nSchwetschenau, S. E., Kovankaya, Y., Elliott, M. A., Allaire, M., White, K. D., & Lall, U. (2023). Optimizing Scale for Decentralized Wastewater Treatment: A Tool to Address Failing Wastewater Infrastructure in the United States. ACS ES&T Engineering, 3(1), 1–14. https://doi.org/10.1021/acsestengg.2c00188"
  },
  {
    "objectID": "readings/week-12-reading.html",
    "href": "readings/week-12-reading.html",
    "title": "Readings for Week 12",
    "section": "",
    "text": "Discussion questions will be posted here.\n\n\n\nAssigned reading will be drawn from:\n\nPollack et al. (2023)\nFletcher et al. (2022)\n\n\n\n\n\nReferences\n\nFletcher, S., Hadjimichael, A., Quinn, J., Osman, K., Giuliani, M., Gold, D., et al. (2022). Equity in Water Resources Planning: A Path Forward for Decision Support Modelers. Journal of Water Resources Planning and Management, 148(7), 02522005. https://doi.org/10.1061/(ASCE)WR.1943-5452.0001573\n\n\nPollack, A., Helgeson, C., Kousky, C., & Keller, K. (2023, September 15). Transparency on underlying values is needed for useful equity measurements. https://doi.org/10.31219/osf.io/kvyxr"
  },
  {
    "objectID": "readings/week-02-reading.html",
    "href": "readings/week-02-reading.html",
    "title": "Week 2 readings",
    "section": "",
    "text": "As before, you are encouraged to use Zotero to manage your references (Zotero has great tools for highlighting and annotating PDFs, among other cool features. It’s also open source so your annotations aren’t locked into a proprietary system).\n\n\n\n\nSeneviratne et al. (2021):\n\nRead the Executive Summary\nRead the Introduction (section 11.1)\nRead the Data and Methods (section 11.2)\nRead one additional subsection (11.3, 11.4, …, or 11.8)\nIf you’re adding this to Zotero, you may find the IPCC-Bibtex project helpful.\n\nRead all of Lall et al. (2018) (pp. 147-157)\nAs you’re reading along, please post any questions you have (clarification, topics you didn’t follow, etc.) to Canvas discussions.\nPrepare a two minute summary of key points from the “one additional subsection” that you read from Seneviratne et al. (2021). Be prepared to share your summary with the class.\nBe prepared to answer basic questions from the readings (focus on the executive summaries)"
  },
  {
    "objectID": "readings/week-02-reading.html#assigned-readings",
    "href": "readings/week-02-reading.html#assigned-readings",
    "title": "Week 2 readings",
    "section": "",
    "text": "As before, you are encouraged to use Zotero to manage your references (Zotero has great tools for highlighting and annotating PDFs, among other cool features. It’s also open source so your annotations aren’t locked into a proprietary system).\n\n\n\n\nSeneviratne et al. (2021):\n\nRead the Executive Summary\nRead the Introduction (section 11.1)\nRead the Data and Methods (section 11.2)\nRead one additional subsection (11.3, 11.4, …, or 11.8)\nIf you’re adding this to Zotero, you may find the IPCC-Bibtex project helpful.\n\nRead all of Lall et al. (2018) (pp. 147-157)\nAs you’re reading along, please post any questions you have (clarification, topics you didn’t follow, etc.) to Canvas discussions.\nPrepare a two minute summary of key points from the “one additional subsection” that you read from Seneviratne et al. (2021). Be prepared to share your summary with the class.\nBe prepared to answer basic questions from the readings (focus on the executive summaries)"
  },
  {
    "objectID": "readings/week-02-reading.html#discussion-questions",
    "href": "readings/week-02-reading.html#discussion-questions",
    "title": "Week 2 readings",
    "section": "Discussion questions",
    "text": "Discussion questions\n\nThink about one impact of climate change you were surprised to learn about and one hazard that is less of a concern than you thought it was.\nWhat are some of the challenges of using climate models to predict future climate? What are some of the challenges of using climate models to predict future impacts of climate change?\nWhat are some of the challenges of using observations to understand future climate? What are some of the challenges of using models to understand future impacts of climate change?"
  },
  {
    "objectID": "readings/week-08-reading.html",
    "href": "readings/week-08-reading.html",
    "title": "Readings for Week 8",
    "section": "",
    "text": "Please read Zarekarizi et al. (2020).\n\nCreate a table of the uncertainties considered, why they are important, and how they are treated in the paper. Be prepared to share your table with the class.\nWhat objectives are considered in the paper? Why might it be difficult to combine these objectives into a single objective function?\nWhat processes or phenomena are omitted from the model? What are the potential impacts of these omissions?\n\n\n\n\n\nReferences\n\nZarekarizi, M., Srikrishnan, V., & Keller, K. (2020). Neglecting uncertainties biases house-elevation decisions to manage riverine flood risks. Nature Communications, 11(1, 1), 5361. https://doi.org/10.1038/s41467-020-19188-9"
  },
  {
    "objectID": "readings/week-11-reading.html",
    "href": "readings/week-11-reading.html",
    "title": "Readings for Week 11",
    "section": "",
    "text": "Discussion questions will be posted here.\n\n\n\nAssigned reading will be drawn from:\n\nSchneider (2002)\nLempert & Schlesinger (2000)\nOreskes et al. (1994)\n\n\n\n\n\nReferences\n\nLempert, R. J., & Schlesinger, M. E. (2000). Robust strategies for abating climate change. Climatic Change, 45(3-4), 387–401. https://doi.org/10.1023/A:1005698407365\n\n\nOreskes, N., Shrader-Frechette, K., & Belitz, K. (1994). Verification, validation, and confirmation of numerical models in the Earth sciences. Science. https://doi.org/10.1126/science.263.5147.641\n\n\nSchneider, S. H. (2002). Can we estimate the likelihood of climatic changes at 2100? Climatic Change, 52(4), 441–451. https://doi.org/http://dx.doi.org/10.1023/A:1014276210717"
  },
  {
    "objectID": "readings/week-06-reading.html",
    "href": "readings/week-06-reading.html",
    "title": "Readings for Week 6",
    "section": "",
    "text": "Please read Bankes (1993)."
  },
  {
    "objectID": "readings/week-06-reading.html#discussion-questions",
    "href": "readings/week-06-reading.html#discussion-questions",
    "title": "Readings for Week 6",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nFor these 2 questions, be ready to share 1-2 sentences for each of them by writing them on the whiteboard at the front at the beginning of the discussion.\n\nWhat is the main idea of this paper?\nWhy did the author write this paper?\n\nThe author outlines 2 main types of modeling (consolidative and exploratory). Be ready to discuss the key differences of both of these main approaches.\nThe author uses a consolidative modeling example of the “ultimate combat model” to illustrate the main issues with consolidative models.\n\nWhat are some of the problems?\nAre there ways to avoid these problems in consolidative modeling?\n\nWhat are the uses of exploratory modeling, according to this paper? (so what?)\nWhat do we do with the uncertainty we discovered in our model?\n\ni.e. what is the role of sensitivity analysis? If we find the bounds in our model, then what?\nFollow up: how would we use model uncertainty for our case study on building elevations?\n\nThe author discusses a hierarchy of exploratory modeling types (data-driven, question-driven, model-driven). Let’s talk about the goals of each of them and how that may lead to differences in our modeling approach."
  },
  {
    "objectID": "readings/week-10-reading.html",
    "href": "readings/week-10-reading.html",
    "title": "Readings for Week 10",
    "section": "",
    "text": "Please read Steinschneider et al. (2015) and consider the following questions:\n\n“Arbitrary” inputs What were some expert-based or arbitrary input values (e.g. failure definition) that were used in the analysis? How can they determine the analysis results and the decision making process?\nLimited actions What are some advantages/disadvantages of using a small and discrete group of actions in comparison to model the actions as continuous variables or even sequential policies.\nUncertainty modeling What were the main sources of uncertainty in the problem, how they were handled and how do they impact the robustness of actions?\nRobustness How is robustness defined by the author for the case study and how is useful within a decision making framework? Think in two or three other case analyses (e.g. our house elevation problem) and propose how to measure the robustness of actions. 1, Computational burden If you could have unlimited computational resources what parts of the analysis would you have done differently or how would you improve the framework?\nStates of the world Besides climate and demand, what are other considerations (sow’s) in the analysis that you think should be included for having a broader picture of the problem for the decision makers? Particularly for such a long-time planning window. Prepare two for sharing with the class.\nLimitations What could be some limitations or potential issues of robustness analysis. Think in one case when robustness might not be very informative to decision makers.\n\n\n\n\n\nReferences\n\nSteinschneider, S., McCrary, R., Wi, S., Mulligan, K., Mearns, L. O., & Brown, C. M. (2015). Expanded decision-scaling framework to select robust long-term water-system plans under hydroclimatic uncertainties. Journal of Water Resources Planning and Management, 141(11). https://doi.org/10.1061/(asce)wr.1943-5452.0000536"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CEVE 421/521: Climate Risk Management",
    "section": "",
    "text": "This is the course website for the Spring 2024 edition of CEVE 421/521, Climate Risk Management, taught at Rice University by James Doss-Gollin.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "CEVE 421/521: Climate Risk Management",
    "section": "Course Information",
    "text": "Course Information\nClimate variability and change pose threats to lives and livelihoods. These climate risks can be managed through the design and operation of infrastructure systems, as well as through disaster response and recovery. Decisions about how to develop and choose risk management strategies are often based on pure vibes, but occasionally rigorous quantitative analyses that make use of scientific information can inform them (we will focus on these cases). These analyses involve integrating knowledge from multiple disciplines to balance competing goals (objectives) under uncertainty.\nIn this course, you will learn climate science, uncertainty quantification, and decision analysis methods to support climate risk management. You will be assigned readings for every class that cover both methods and applications, and will work collaboratively to implement key concepts through programming problem sets. Active class participation is required. Methods covered include scenario analysis, exploratory modeling, cost-benefit analysis, single- and multi-objective policy search, reinforcement learning, deep uncertainty, robust decision making, and equitable decision making.\nFor additional information, see the syllabus.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "CEVE 421/521: Climate Risk Management",
    "section": "Instructor",
    "text": "Instructor\nDr. James Doss-Gollin is an assistant professor of Civil and Environmental Engineering at Rice University. His research integrates Earth science, data science, and decision science to address challenges in climate risk management, water resources, and energy system resilience. He also teaches CEVE 543 (Data Science for Climate Hazards).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#software-tools",
    "href": "index.html#software-tools",
    "title": "CEVE 421/521: Climate Risk Management",
    "section": "Software Tools",
    "text": "Software Tools\n\nThis course will use the Julia programming language. Julia is a modern, free, open source language designed for scientific computing.\nNo prior knowledge of Julia is expected, but some exposure to programming is strongly encouraged.\nAssignments will be distributed using GitHub Classroom.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "CEVE 421/521: Climate Risk Management",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe layout for this site was inspired by and draws from Vivek Srikrishnan’s Environmental Systems Analysis course at Cornell, STA 210 at Duke University, and Andrew Heiss’s course materials at Georgia State. It builds heavily from my data science for climate hazard assessment course, CEVE 543.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "labs/lab-02/template.html",
    "href": "labs/lab-02/template.html",
    "title": "Lab 2: Julia Quickstart",
    "section": "",
    "text": "We start by loading the packages we will use in this lab\n\nusing CSV\nusing DataFrames\nusing DataFramesMeta\nusing Dates\nusing Plots\nusing StatsBase: mean\nusing StatsPlots\nusing Unitful"
  },
  {
    "objectID": "labs/lab-02/template.html#first-steps",
    "href": "labs/lab-02/template.html#first-steps",
    "title": "Lab 2: Julia Quickstart",
    "section": "",
    "text": "We start by loading the packages we will use in this lab\n\nusing CSV\nusing DataFrames\nusing DataFramesMeta\nusing Dates\nusing Plots\nusing StatsBase: mean\nusing StatsPlots\nusing Unitful"
  },
  {
    "objectID": "labs/lab-02/template.html#defining-a-function",
    "href": "labs/lab-02/template.html#defining-a-function",
    "title": "Lab 2: Julia Quickstart",
    "section": "Defining a function",
    "text": "Defining a function\nIn index.qmd, we read in a CSV file from scratch. However, we’d like to repeat this process for each year of data, and to do it in a consistent way so that we can read in the data for all available years into a single file. To do this, we’ll write a function that we can use to read in the data for any year. Specifically, our function will take in the year as an argument, and return a DataFrame with the data for that year.\nBefore we do that, let’s define a function that will return the filename for a given year. It’s often valuable to stack several functions together.\n\nget_fname(year::Int) = \"data/tidesandcurrents-8638610-$(year)-NAVD-GMT-metric.csv\"\n\nNow we’re ready to define our function:\n\nfunction read_tides(year::Int)\n    \n    # define the CSV file corresponding to our year of choice\n    fname = get_fname(year)\n\n    # a constant, don't change this\n    date_format = \"yyyy-mm-dd HH:MM\"\n    \n    # &lt;YOUR CODE GOES HERE&gt;\n    # 1. read in the CSV file and save as a dataframe\n    # 2. convert the \"Date Time\" column to a DateTime object\n    # 3. convert the \" Water Level\" column to meters\n    # 4. rename the columns to \"datetime\" and \"lsl\"\n    # 5. select the \"datetime\" and \"lsl\" columns\n    # 6. return the dataframe\nend\n\n# print out the first 10 rows of the 1928 data\nfirst(read_tides(1928), 10) \n\n\n\n\n\n\nInstructions\n\n\n\nFill out this function. Your function should implement the six steps indicated in the instructions. Use the example code from index.qmd to help you. When it’s done, convert it to a live code block by replacing ```julia``` with ```{julia}```. When you run this code, it should print out the first 10 rows of the 1928 data. Make sure they look right!"
  },
  {
    "objectID": "labs/lab-02/template.html#building-the-dataset",
    "href": "labs/lab-02/template.html#building-the-dataset",
    "title": "Lab 2: Julia Quickstart",
    "section": "Building the dataset",
    "text": "Building the dataset\nNow that we have the ability to read in the data corresponding to any year, we can read them all in and combine into a single DataFrame. First, let’s read in all the data.\n\n\n\n\n\n\nInstructions\n\n\n\n\nHint: to vectorize a function means to apply it to each element of a vector. For example, f.(x) will apply the function f to each element of the vector x. This is a very common operation in Julia!\nUpdate the code blocks below, then replace ```julia``` with ```{julia}```.\n\n\n\nyears = 1928:2021 # all the years of data\nannual_data = # call the read_tides function on each year (see hint above!)\ntypeof(annual_data) # should be a vector of DataFrames\nNext, we’ll use the vcat function to combine all the data into a single DataFrame.\ndf = vcat(annual_data...)\nfirst(df, 5)\nAnd we can look at the last 5 rows\nlast(df, 5)\nFinally, we’ll make sure we drop any missing data.\ndropmissing!(df) # drop any missing data"
  },
  {
    "objectID": "labs/lab-02/template.html#plots",
    "href": "labs/lab-02/template.html#plots",
    "title": "Lab 2: Julia Quickstart",
    "section": "Plots",
    "text": "Plots\n\nPlot the hourly water levels for March 2020, using subsetting and plotting techniques from the instructions\nIn the instructions, we plotted the average monthly water level from each month using groupby. Repeat this analysis, using the full dataset (all years).\nNow repeat the analysis, but group by day of the year. What do you notice? (Hint: use Dates.dayofyear to get the day of the year from a DateTime object)"
  },
  {
    "objectID": "labs/lab-05/template.html",
    "href": "labs/lab-05/template.html",
    "title": "Lab 5: Sea-Level Rise",
    "section": "",
    "text": "As always:\n\nClone the lab repository to your computer\nOpen the lab repository in VS Code\nOpen the Julia REPL and activate, then instantiate, the lab environment\nMake sure you can render: quarto render template.qmd in the terminal.\n\nIf you run into issues, try running ] build IJulia in the Julia REPL (] enters the package manager).\nIf you still have issues, try opening up blankfile.py. That should trigger VS Code to give you the option to install the Python extension, which you should do. Then you should be able to open a menu in the bottom right of your screen to select which Python installation you want VS Code to use.\n\n\n\n\n\n\nusing CSV\nusing DataFrames\nusing DataFramesMeta\nusing Distributions\nusing Plots\nusing StatsPlots\nusing Unitful\n\nPlots.default(; margin=5Plots.mm)\n\n\n\n\n\nusing Revise\nusing HouseElevation"
  },
  {
    "objectID": "labs/lab-05/template.html#the-usual",
    "href": "labs/lab-05/template.html#the-usual",
    "title": "Lab 5: Sea-Level Rise",
    "section": "",
    "text": "As always:\n\nClone the lab repository to your computer\nOpen the lab repository in VS Code\nOpen the Julia REPL and activate, then instantiate, the lab environment\nMake sure you can render: quarto render template.qmd in the terminal.\n\nIf you run into issues, try running ] build IJulia in the Julia REPL (] enters the package manager).\nIf you still have issues, try opening up blankfile.py. That should trigger VS Code to give you the option to install the Python extension, which you should do. Then you should be able to open a menu in the bottom right of your screen to select which Python installation you want VS Code to use."
  },
  {
    "objectID": "labs/lab-05/template.html#load-packages",
    "href": "labs/lab-05/template.html#load-packages",
    "title": "Lab 5: Sea-Level Rise",
    "section": "",
    "text": "using CSV\nusing DataFrames\nusing DataFramesMeta\nusing Distributions\nusing Plots\nusing StatsPlots\nusing Unitful\n\nPlots.default(; margin=5Plots.mm)"
  },
  {
    "objectID": "labs/lab-05/template.html#local-package",
    "href": "labs/lab-05/template.html#local-package",
    "title": "Lab 5: Sea-Level Rise",
    "section": "",
    "text": "using Revise\nusing HouseElevation"
  },
  {
    "objectID": "labs/lab-04/template.html",
    "href": "labs/lab-04/template.html",
    "title": "Lab 4: House Elevation NPV Analysis",
    "section": "",
    "text": "using CSV\nusing DataFrames\nusing DataFramesMeta\nusing Distributions\nusing Interpolations\nusing Plots\nusing StatsPlots\nusing Unitful\n\nPlots.default(; margin=6Plots.mm)"
  },
  {
    "objectID": "labs/lab-03/template.html",
    "href": "labs/lab-03/template.html",
    "title": "Lab 3: Depth-Damage Models",
    "section": "",
    "text": "using CSV\nusing DataFrames\nusing DataFramesMeta\nusing Distributions\nusing Interpolations\nusing Plots\nusing StatsPlots\nusing Unitful\n\nPlots.default(; margin=6Plots.mm)\n\n\nSite information\n\n\nDepth-Damage\n\n\nExpected annual damages\n\n\nDiscussion"
  },
  {
    "objectID": "labs/lab-08/index.html",
    "href": "labs/lab-08/index.html",
    "title": "Lab 8: The Lake Problem",
    "section": "",
    "text": "This is a placeholder."
  },
  {
    "objectID": "labs/lab-08/index.html#overview",
    "href": "labs/lab-08/index.html#overview",
    "title": "Lab 8: The Lake Problem",
    "section": "",
    "text": "This is a placeholder."
  },
  {
    "objectID": "labs/lab-06/index.html",
    "href": "labs/lab-06/index.html",
    "title": "Lab 6: Policy Search",
    "section": "",
    "text": "In this lab, we will implement single-objective policy search for our house elevation problem. These methods can also be used for multi-objective policy search, but this does increase computational complexity."
  },
  {
    "objectID": "labs/lab-06/index.html#setup",
    "href": "labs/lab-06/index.html#setup",
    "title": "Lab 6: Policy Search",
    "section": "Setup",
    "text": "Setup\nAs always:\n\nClone the lab repository to your computer\nOpen the lab repository in VS Code\nOpen the Julia REPL and activate, then instantiate, the lab environment\nMake sure you can render: quarto render template.qmd in the terminal.\n\nIf you run into issues, try running ] build IJulia in the Julia REPL (] enters the package manager).\nIf you still have issues, try opening up blankfile.py. That should trigger VS Code to give you the option to install the Python extension, which you should do. Then you should be able to open a menu in the bottom right of your screen to select which Python installation you want VS Code to use.\n\n\n\nusing CSV\nusing DataFrames\nusing DataFramesMeta\nusing Distributions\nusing LaTeXStrings\nusing Metaheuristics\nusing Plots\nusing Random\nusing Unitful\n\nPlots.default(; margin=5Plots.mm)\n\nWe also load our local package as in lab 5.\n\nusing Revise\nusing HouseElevation\n\nPrecompiling HouseElevation\n  ✓ HouseElevation\n  1 dependency successfully precompiled in 2 seconds. 82 already precompiled."
  },
  {
    "objectID": "labs/lab-06/index.html#decision-variables",
    "href": "labs/lab-06/index.html#decision-variables",
    "title": "Lab 6: Policy Search",
    "section": "Decision variables",
    "text": "Decision variables\nWe’re going to focus on a single decision variable: how high to elevate a house. Of course, running a full optimization here is probably overkill, as Zarekarizi et al. (2020) showed that a brute force search over all possible elevations is sufficient to find a good solution. However, we want to build up some optimization expertise to help us with more complex problems in the future.\nWe will use a continuous decision variable, the height of the house above the ground. We limit it between 0 and 14 feet."
  },
  {
    "objectID": "labs/lab-06/index.html#objective-function",
    "href": "labs/lab-06/index.html#objective-function",
    "title": "Lab 6: Policy Search",
    "section": "Objective function",
    "text": "Objective function\nFor now, we’ll keep the same objective function that we’ve been using: net present value, considering both the cost of heightening the house and the discounted expected costs of future flood damage.\nAs you know, it’s not enough to state the objective function, however. We also need to consider the state(s) of the world over which we will optimize.\n\nslr_scenarios = let\n    df = CSV.read(\"data/slr_oddo.csv\", DataFrame)\n    [Oddo17SLR(a, b, c, tstar, cstar) for (a, b, c, tstar, cstar) in eachrow(df)]\nend\n\nhouse = let\n    haz_fl_dept = CSV.read(\"data/haz_fl_dept.csv\", DataFrame) # read in the file\n    desc = \"one story, Contents, fresh water, short duration\"\n    row = @rsubset(haz_fl_dept, :Description == desc)[1, :] # select the row I want\n    area = 500u\"ft^2\"\n    height_above_gauge = 10u\"ft\"\n    House(row; area=area, height_above_gauge=height_above_gauge, value_usd=250_000)\nend\n\np = ModelParams(; house=house, years=2024:2083)\n\nfunction draw_surge_distribution()\n    μ = rand(Normal(5, 1))\n    σ = rand(Exponential(1.5))\n    ξ = rand(Normal(0.1, 0.05))\n    return GeneralizedExtremeValue(μ, σ, ξ)\nend\nfunction draw_discount_rate()\n    return 0.0\nend\n\nN_SOW = 100\nsows = [\n    SOW(rand(slr_scenarios), draw_surge_distribution(), draw_discount_rate()) for\n    _ in 1:N_SOW\n] # for 10 SOWs\n\n\nMore-efficient storm surge sampling\nThus far, we have estimated annual expected losses in a particular year using a simple Monte Carlo estimate: \\[\n\\mathbb{E}[f(x)] = \\int p(x) f(x) \\, dx \\quad \\approx  \\frac{1}{N} \\sum_{i=1}^N f(x_i) \\quad \\text{where } x_i \\sim p(x)\n\\] where, in our context, \\(x\\) is the flood depth and \\(f(x)\\) is the depth-damage function. However, this is not the most efficient way to estimate the expected value of a function, because it requires a lot of samples from \\(p(x)\\) to get a good estimate. This will invariably result in a lot of samples very near each other in the middle of the distribution, where the function is relatively flat, and few samples in the tails, where the function is steep.\nA more efficient way to estimate the expected value of a function is to use trapezoidal integration. This is a simple idea: we divide the domain of the function into \\(n\\) intervals, and then estimate the area under the curve in each interval using the trapezoidal rule: \\[\n\\int_{a}^b f(x) \\, dx \\approx \\sum_{i=1}^{n-1} \\frac{f(x_i) + f(x_{i+1})}{2} (x_{i+1} - x_i)\n\\] where \\(x_i\\) are the \\(n\\) points at which we evaluate the function between \\(a\\) and \\(b\\). If we want to estimate an expectation, we need to multiply this by the probability of each interval, which we can estimate using the empirical distribution of our samples. \\[\n\\mathbb{E}[f(x)] \\approx \\sum_{i=1}^{n-1} \\frac{p(x_i)f(x_i) + p(x_{i+1}) f(x_{i+1})}{2} (x_{i+1} - x_i).\n\\] One important note here is that an expectation is infinite, but we truncate the integral at some point. We will use the 0.0005 and 0.9995 quantiles of the distribution of flood depths to define the bounds of the integral, which introduces some (here very small) error.\n\n\nValidation\nWe can make sure that we get the same thing by comparing our result to a simple Monte Carlo estimate using 25,000 samples. We can also time how long they take using the @time macro (which is actually not a great way to evaluate code – a better approach uses the BenchmarkTools package but we won’t go into this here – instead we run use @time the second time we call each function, which gives a better estimate than timing it the first time).\n\na = Action(3.0u\"ft\")\nsow = first(sows)\n_ = run_sim(a, sow, p)\n@time run_sim(a, sow, p)\n\n  0.000852 seconds (1.48 k allocations: 577.188 KiB)\n\n\n-61567.0\n\n\nThis is much faster than the old method, but gives us the same result\n\n_ = HouseElevation.run_sim_old(a, sow, p)\n@time HouseElevation.run_sim_old(a, sow, p)\n\n  0.081204 seconds (1.17 k allocations: 45.819 MiB)\n\n\n-61567.0\n\n\nThis makes our function a lot faster to run! We only have to evalute our function about 150 times per year, rather than 10000."
  },
  {
    "objectID": "labs/lab-06/index.html#metaheuristics.jl",
    "href": "labs/lab-06/index.html#metaheuristics.jl",
    "title": "Lab 6: Policy Search",
    "section": "Metaheuristics.jl",
    "text": "Metaheuristics.jl\nWe are attempting to solve a single-objective optimization problem that is likely nonlinear and nonconvex. We will use the Metaheuristics.jl package to do this. This package implements a number of optimization algorithms, including genetic algorithms, that are well-suited to this type of problem. Let’s follow a a quick overview from the docs.\nLet’s say we want to minimize the following function: \\[\nf(\\mathbf{x}) = 10D + \\sum_{i=1}^{D} \\left( x_i^2 - 10 \\cos(2 \\pi x_i) \\right)\n\\] where \\(\\mathbf{x} \\in [-5, 5]^D\\), i.e., \\(-5 \\leq x_i \\leq 5\\) for \\(i = 1, \\ldots, D\\). We can plot it for \\(D=2\\):\n\nf(x) = 10length(x) + sum(x .^ 2 - 10cos.(2π * x))\nlet\n    # Generate a grid of points for the surface plot\n    x = range(-5; stop=5, length=1000)\n    y = range(-5; stop=5, length=1000)\n    z = [f([i, j]) for i in x, j in y]\n\n    # Create the surface plot\n    surface(\n        x, y, z; xlabel=\"x1\", ylabel=\"x2\", zlabel=\"f(x)\", title=L\"Minimize $f(x)$ for $D=2$\"\n    )\nend\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\nLet’s minimize it with \\(D=10\\). We need to define bounds of the optimization, which constrains the search space for the decisino variable.\n\nD = 10\nbounds = boxconstraints(; lb=-5ones(D), ub=5ones(D))\n\nBoxConstrainedSpace{Float64}([-5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0, -5.0], [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0], [10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0], 10, true)\n\n\nWe can throw this straight into the optimize function:\n\nresult = optimize(f, bounds)\n\nOptimization Result\n===================\n  Iteration:       689\n  Minimum:         2.98488\n  Minimizer:       [2.40746e-09, -1.65436e-09, -5.23711e-10, …, -1.21936e-09]\n  Function calls:  48230\n  Total time:      0.0643 s\n  Stop reason:     Due to Convergence Termination criterion.\n\n\n\nWe can view the minimum of the objective function with\n\nminimum(result)\n\n2.9848771712798623\n\n\nand the value of the decision variable that achieves that minimum with:\n\nminimizer(result)\n\n10-element Vector{Float64}:\n  2.407455945328395e-9\n -1.6543566344147237e-9\n -5.237105122081643e-10\n -0.9949586386537573\n -5.870011848871736e-10\n  1.0496941270878932e-9\n -0.9949586371209773\n -0.9949586391333252\n -5.184639026512626e-10\n -1.2193556291216736e-9\n\n\nOften, however, some additional specifications are needed. In our case, we’ll want to add some specifications for the optimizer. For example, we can set a time limit (in seconds) for the optimization – this may cause it to terminate before convergence! Another useful option is f_tol_rel, which sets the relative tolerance for the objective function value. For our case study, we’ll want a very loose tolerance (high value of f_tol_rel) because the function we are minimizing has results on the order of tens of thousands of dollars. For this tutorial, we’ll use the default value of f_tol_rel.\n\noptions = Options(; time_limit=10.0)\n\n\nOptions\n=======\n  rng:             TaskLocalRNG()\n  seed:            955667554\n  x_tol:           1.0e-8\n  f_tol:           1.0e-12\n  g_tol:           0.0\n  h_tol:           0.0\n  debug:           false\n  verbose:         false\n  f_tol_rel:       2.220446049250313e-16\n  time_limit:      10.0\n  iterations:      0\n  f_calls_limit:   0.0\n  store_convergence: false\n  parallel_evaluation: false\n\n\n\n\nTo use options, we have to choose an algorithm. See list of algorithms here. The ECA algorithm is suggested as a default, so we’ll use that.\n\nalgorithm = ECA(; options=options)\n\n\nAlgorithm Parameters\n====================\n  ECA(η_max=2.0, K=7, N=0, N_init=0, p_exploit=0.95, p_bin=0.02, ε=0.0, adaptive=false, resize_population=false)\nOptimization Result\n===================\n  Empty status.\n\n\n\n\nBefore we run the optimization, let’s set a random seed. This will make our results more reproducible. We can then vary the seed to see how sensitive our results are to the random seed.\n\nRandom.seed!(918)\nresult = optimize(f, bounds, algorithm)\n\nOptimization Result\n===================\n  Iteration:       822\n  Minimum:         5.96975\n  Minimizer:       [-0.994959, -0.994959, -1.89236e-09, …, -2.9115e-09]\n  Function calls:  57540\n  Total time:      0.0866 s\n  Stop reason:     Due to Convergence Termination criterion.\n\n\n\nand we can check if we get a different result in the next iteration\n\nRandom.seed!(952)\nresult = optimize(f, bounds, algorithm)\n\nOptimization Result\n===================\n  Iteration:       1\n  Minimum:         5.96975\n  Minimizer:       [-0.994959, -0.994959, -1.20158e-09, …, 1.425e-09]\n  Function calls:  70\n  Total time:      0.0400 s\n  Stop reason:     Due to Convergence Termination criterion."
  },
  {
    "objectID": "labs/lab-06/index.html#set-up-caching",
    "href": "labs/lab-06/index.html#set-up-caching",
    "title": "Lab 6: Policy Search",
    "section": "Set up caching",
    "text": "Set up caching\nYou’ll want to pip install jupyter-cache and then make sure you have caching enabled for this one, since some of the comnputations are a bit slow and you don’t want to have to re-run them every time you open VS Code."
  },
  {
    "objectID": "labs/lab-06/index.html#explore",
    "href": "labs/lab-06/index.html#explore",
    "title": "Lab 6: Policy Search",
    "section": "Explore",
    "text": "Explore\nBefore digging too deep into the case study, play around with some of the parameters in this optimization tutorial. Vary \\(D\\), the bounds of the optimization problem, the stopping criteria, or the algorithm. Get some intuition and ask any questions you have about the optimization process."
  },
  {
    "objectID": "labs/lab-06/index.html#optional-improved-estimation-of-flood-pdf",
    "href": "labs/lab-06/index.html#optional-improved-estimation-of-flood-pdf",
    "title": "Lab 6: Policy Search",
    "section": "Optional: improved estimation of flood PDF",
    "text": "Optional: improved estimation of flood PDF\nIf you’d like, you can use Extremes.jl (or another method you’re familiar with) and the gauge data for the nearest station. See previous instructions for tips on the data. You can use this to make your case study more realistic. If you don’t want to do this, you can continue to use your hypothetical distribution."
  },
  {
    "objectID": "labs/lab-06/index.html#optimization",
    "href": "labs/lab-06/index.html#optimization",
    "title": "Lab 6: Policy Search",
    "section": "Optimization",
    "text": "Optimization\nIn order to use this optimization package on our problem, we need to define an objective function. This includes not only the objective, but also the SOW(s) over which we will optimize. This also introduces a trade-off: using a few SOWs will make the optimization faster, but may result in a suboptimal solution. Using many SOWs will make the optimization slower, but may result in a more robust solution.\nWe’ll keep the number of SOWs used for optimization relatively small, and then we’ll evaluate performance (of our “optimal” solution) using a larger number of SOWs.\n\nSet your random seed to 2024 so that you always get the same answers when you re-run your code.\nGenerate N_SOW = 100_000 sows at random as in the previous lab and/or as in the template code provided above.\nPick the first N_SOW_opt = 10 of these sows to use for optimization. You can (and should!!) increase this number once you have a working solution, but we’ll use just a few to make sure everything is working.\nDefine an objective function that takes in a single number as an input (the elevation of the house in feet) and returns one objective function (the net present value of the house for that elevation).\n\nConvert the input scalar to an Action\nCall run_sim on each of the N_SOW_opt sows and the elevation to get the expected value of the objective function.\nReturn the negative of the sum of these expected values (since we are minimizing).\n\nTest your objective function with a few different elevations to make sure it’s working.\nRun the optimization with the objective function and see what elevation it recommends.\nValidate the result by plotting the objective function for a range of elevations (from 0 to 14 ft) using all your SOWs. Is the recommended elevation the minimum? (We’re lucky that in this problem we can compare our optimization solution to a brute-force approach!) If it doesn’t seem to be the minimum:\n\ntry increasing N_SOW_opt and see if the result changes.\ncheck whether the optimization algorithm is converging"
  },
  {
    "objectID": "labs/lab-06/index.html#reflection",
    "href": "labs/lab-06/index.html#reflection",
    "title": "Lab 6: Policy Search",
    "section": "Reflection",
    "text": "Reflection\nConclude your analysis by reflecting on the following questions\n\nHow are we framing this problem? What are the decision variables, the objective function, and the states of the world over which we optimize?\nDiggning deeper, we are averaging the objective function computed over a finite number of states of the world. This assumes that they are all drawn from a distribution representing the “true” distribution of states of the world. Is this a good assumption?\nWhat’s not being considered in this analysis that might be important?"
  },
  {
    "objectID": "labs/lab-07/template.html",
    "href": "labs/lab-07/template.html",
    "title": "Lab 7: Parking Garage Case Study",
    "section": "",
    "text": "using Revise\nusing ParkingGarage\n\nand also regular packages\n\nusing Plots\nPlots.default(; margin=5Plots.mm)"
  },
  {
    "objectID": "labs/lab-07/index.html",
    "href": "labs/lab-07/index.html",
    "title": "Lab 7: Parking Garage Case Study",
    "section": "",
    "text": "Neufville et al. (2006) introduced a case study of a parking garage in which the decision variable is the number of levels to build. This is about as simple as a sequential decision problem can get, which makes it a great “toy problem” to illustrate the basic concepts of sequential decision making and how to program them effectively.\n\n\nAs always:\n\nClone the lab repository to your computer\nOpen the lab repository in VS Code\nOpen the Julia REPL and activate, then instantiate, the lab environment\nMake sure you can render: quarto render template.qmd in the terminal.\n\nIf you run into issues, try running ] build IJulia in the Julia REPL (] enters the package manager).\nIf you still have issues, try opening up blankfile.py. That should trigger VS Code to give you the option to install the Python extension, which you should do. Then you should be able to open a menu in the bottom right of your screen to select which Python installation you want VS Code to use.\n\n\n\n\n\n\nusing Revise\nusing ParkingGarage\n\nPrecompiling ParkingGarage\n  ✓ ParkingGarage\n  1 dependency successfully precompiled in 1 seconds\n\n\nand also regular packages\n\nusing Plots\nPlots.default(; margin=5Plots.mm)\n\n\n\n\nWe view the problem as a sequential decision problem following Neufville et al. (2006). We have a single decision to make: how many levels to build. We will compare results for two cases:\n\nstatic case. The number of levels is fixed.\nadaptive case. We pay an extra 5% for up-front costs, but then retain the option to build more levels in the future. We will use a simple rule to decide when to build more levels: if demand exceeds the current capacity, we will build one more level.\n\nAs we’ve seen in class, a key concept in sequential decision making is the idea of a state. In this problem, we have three state variables: the year and the number of levels. We could add some complexity to our problem by making the demand stochastic, in which case we’d want it to be a state variable, but here we’ll treat it as a determinstic function of time.\nWe also have some uncertainty in our model: the discount rate, the time horizon, and the demand growth rate. The paper uses an exponential growth model for demand, but we’ll use a linear one.\n\n\nCode\nlet\n    sow = ParkingGarageSOW()\n    years = 1:(sow.n_years)\n    demand = [\n        ParkingGarage.calculate_demand(year, sow.demand_growth_rate) for year in years\n    ]\n    plot(\n        years,\n        demand;\n        ylabel=\"Demand [cars/day]\",\n        xlabel=\"Year\",\n        legend=false,\n        title=\"Demand Growth Rate: $(sow.demand_growth_rate) Cars/Year\",\n        size=(800, 400),\n        marker=:circle,\n    )\nend"
  },
  {
    "objectID": "labs/lab-07/index.html#setup",
    "href": "labs/lab-07/index.html#setup",
    "title": "Lab 7: Parking Garage Case Study",
    "section": "",
    "text": "As always:\n\nClone the lab repository to your computer\nOpen the lab repository in VS Code\nOpen the Julia REPL and activate, then instantiate, the lab environment\nMake sure you can render: quarto render template.qmd in the terminal.\n\nIf you run into issues, try running ] build IJulia in the Julia REPL (] enters the package manager).\nIf you still have issues, try opening up blankfile.py. That should trigger VS Code to give you the option to install the Python extension, which you should do. Then you should be able to open a menu in the bottom right of your screen to select which Python installation you want VS Code to use."
  },
  {
    "objectID": "labs/lab-07/index.html#load-packages",
    "href": "labs/lab-07/index.html#load-packages",
    "title": "Lab 7: Parking Garage Case Study",
    "section": "",
    "text": "using Revise\nusing ParkingGarage\n\nPrecompiling ParkingGarage\n  ✓ ParkingGarage\n  1 dependency successfully precompiled in 1 seconds\n\n\nand also regular packages\n\nusing Plots\nPlots.default(; margin=5Plots.mm)"
  },
  {
    "objectID": "labs/lab-07/index.html#formal-problem-framing",
    "href": "labs/lab-07/index.html#formal-problem-framing",
    "title": "Lab 7: Parking Garage Case Study",
    "section": "",
    "text": "We view the problem as a sequential decision problem following Neufville et al. (2006). We have a single decision to make: how many levels to build. We will compare results for two cases:\n\nstatic case. The number of levels is fixed.\nadaptive case. We pay an extra 5% for up-front costs, but then retain the option to build more levels in the future. We will use a simple rule to decide when to build more levels: if demand exceeds the current capacity, we will build one more level.\n\nAs we’ve seen in class, a key concept in sequential decision making is the idea of a state. In this problem, we have three state variables: the year and the number of levels. We could add some complexity to our problem by making the demand stochastic, in which case we’d want it to be a state variable, but here we’ll treat it as a determinstic function of time.\nWe also have some uncertainty in our model: the discount rate, the time horizon, and the demand growth rate. The paper uses an exponential growth model for demand, but we’ll use a linear one.\n\n\nCode\nlet\n    sow = ParkingGarageSOW()\n    years = 1:(sow.n_years)\n    demand = [\n        ParkingGarage.calculate_demand(year, sow.demand_growth_rate) for year in years\n    ]\n    plot(\n        years,\n        demand;\n        ylabel=\"Demand [cars/day]\",\n        xlabel=\"Year\",\n        legend=false,\n        title=\"Demand Growth Rate: $(sow.demand_growth_rate) Cars/Year\",\n        size=(800, 400),\n        marker=:circle,\n    )\nend"
  },
  {
    "objectID": "labs/lab-07/index.html#uncertainty",
    "href": "labs/lab-07/index.html#uncertainty",
    "title": "Lab 7: Parking Garage Case Study",
    "section": "Uncertainty",
    "text": "Uncertainty\nFigure 1 of Neufville et al. (2006) shows how the NPV changes when uncertainty is added to the model. Reproduce this figure, using our model. Specifically:\n\nGenerate an ensemble of SOWs. Justify how you are sampling the three parameters (n_years, demand_growth_rate, and discount_rate). I suggest to keep n_years as a constant, and perhaps to keep the discount rate constant as well.\nFor each SOW, calculate the NPV for each policy.\nCalculate the average NPV for each number of levels and plot."
  },
  {
    "objectID": "labs/lab-01/index.html",
    "href": "labs/lab-01/index.html",
    "title": "Lab 1: Software Installation",
    "section": "",
    "text": "Labs are in-class exercises intended to get practice with coding or analysis workflows.\n\nInstructions available on website\nDownload ahead of time by using link from Canvas\nYou will have your own repository (more in a minute)\nTry to finish in class, but due in 1 week"
  },
  {
    "objectID": "labs/lab-01/index.html#labs",
    "href": "labs/lab-01/index.html#labs",
    "title": "Lab 1: Software Installation",
    "section": "",
    "text": "Labs are in-class exercises intended to get practice with coding or analysis workflows.\n\nInstructions available on website\nDownload ahead of time by using link from Canvas\nYou will have your own repository (more in a minute)\nTry to finish in class, but due in 1 week"
  },
  {
    "objectID": "labs/lab-01/index.html#toolkit-overview",
    "href": "labs/lab-01/index.html#toolkit-overview",
    "title": "Lab 1: Software Installation",
    "section": "Toolkit: overview",
    "text": "Toolkit: overview\nGetting set up for this course requires the following steps. If you are an experienced programmer, you are free to follow your own workflow to set up these tools. You will absolutely need Quarto, GitHub, and Julia. If you are not an experienced programmer, the following steps are not the only way to get these tools set up, but they are a very good way.\nIf you install course tools using steps other than the ones provided on this page, be aware that your instructors may be able to provide you with only limited support.\n\nJulia\nWe’re using the Julia programming language in this course. Julia is a fast, modern, open-source programming language designed for numerical and scientific computing.\nWe’re using it for a few key reasons. First, the syntax is human-readable and closely parallels math notation, which reduces the cognitive burden of translating between conceptual and computational models. Second, it’s fast, which means that Julia solves the “two language problem”: you don’t need to learn C or Fortran to dig under the hood and write fast code. Other great features include that it’s open-source, which makes it reproducible and shareable, and it has a fantastic package manager.\nThere are some great resources out there about why Julia is great. See posts by Julia Data Science or the Julia Creators (with followup).\n\n\nGitHub\ngit is a software tool for version control that keeps track of changes to files over time. GitHub is a website that hosts git repositories and provides a web interface for interacting with them.\nTo use GitHub, you’ll need a GitHub account. Code on GitHub is stored in repositories. A simple workflow is to clone a repository to your computer, make changes, commit them, then push your changes to GitHub.\nWe will also use GitHub classroom, which allows instructors to share templates and view your code.\n\n\nQuarto\nQuarto is a tool that allows you to combine text and code and create many types of output. For example, this website is made with Quarto! You will use Quarto to create reports for labs. This lets you keep everything in one place – no more running code, saving a figure to Downloads, copying into Word, then trying to remember where to paste the figure when you update the code.\n\n\nVS Code\nVS Code is a text editor. If you are an advanced user of another text editor, you can use that instead. However, VS Code is very nearly an officially supported IDE for Julia."
  },
  {
    "objectID": "labs/lab-01/index.html#installing-software",
    "href": "labs/lab-01/index.html#installing-software",
    "title": "Lab 1: Software Installation",
    "section": "Installing software",
    "text": "Installing software\n\nInstall Julia\nThe best way to install Julia is through the juliaup tool, which will let you easily manage versions in the future and works seamlessly with VS Code. The instructions can be found at the JuliaUp GitHub repository, but we will summarize them here.\nIf your computer uses Windows, you can install Juliaup from the Windows Store.\nIf you have a Mac (or are using Linux), open a terminal (such as the Terminal app) and enter:\ncurl -fsSL https://install.julialang.org | sh\nOnce you install Juliaup, install Julia version 1.10 by opening a terminal (in MacOS or Linux) or the command line (in Windows) and entering:\njuliaup add 1.10\njuliaup default 1.10\nThis will install Julia 1.10 and make it the default version, which should maximize package compatibility throughout this course. Going forward, if you want to add new versions or change the default, you can follow the Juliaup instructions.\n\n\nInstall VS Code\nVS Code is as close to an officially supported editor for Julia as you can get. We will follow this guide for setting up VS Code with Julia.\n\nYou can skip this section if you are an experienced programmer and already have a preferred IDE. Your IDE will likely have instructions for Julia and Quarto setup.\n\nYou can download VS Code here; open the downloaded file to install. Make sure to select the correct version for your operating system. If you have a newish Apple mac (with M1, M2, or M3 chip), make sure to check whether you have an Intel or Apple chip before choosing which version to download. You can also use homebrew or your preferred package manager to install VS Code.\n\nVS Code Julia Extension\nLike many IDEs, VS Code is a modular system that can be extended with plugins. We will install the Julia extension, which will allow us to run Julia code and interact with the Julia REPL from within VS Code (we’ll add the Quarto extension later).\n\nOpen VS Code.\nSelect View and click Extensions to open the Extension View.\nSearch for julia in the search box. Click the green install button.\nRestart VS Code once the installation is complete. It should automatically find your Julia installation; reach out if not.\n\nThe Julia VS Code extension offers you some nice features. You can start a REPL (an interactive Julia coding environment) by opening the “Command Palette” (View -&gt; Command Palette, or CTRL/CMD+SHIFT+P) and typing “REPL” to bring up “Julia: Start REPL”. You can also create .jl and .qmd files to write Julia code and execute line by line.\n\n\n\nGitHub\nSee GitHub official tutorials for more helpful resources and tutorials.\n\nCreate GitHub Account\nIf you already have a GitHub account, you can use that for this course and do not need to create a new account. Otherwise, create an account. It doesn’t have to be linked to your Rice email or your NetID.\nFor labs and projects, you should use the GitHub Classroom link posted on Canvas to “accept” the assignment, which will give you your own GitHub repository for that assignment. The first time you click one of these links, you will need to link your place on the course roster with your GitHub account.\n\n\nGitHub Desktop (Optional)\nYou can do everything that you will need to do for this course with GitHub directly through VS Code. The GitHub desktop app is also great, or alternatively you may work directly through the terminal if you have prior experience.\n\n\n\nGitHub Copilot (Optional)\nGitHub Copilot is an AI-powered tool that helps you write code. You can install it following instructions here As described in the quickstart guide, Copilot is free for students. Be sure to review the policy on AI language models in the syllabus!\n\nInstall Git\ngit is a version control software that powers GitHub under the hood (git is the version control software, GitHub is an online platform). Based on past experience with the course, you probably already have git installed! If you’re not sure if it’s installed, see instructions here.\n\n\n\nInstall Quarto\nQuarto combines the best of Jupyter notebooks and R Markdown to create a document format that is ideal for conducting and communicating data science. We will use Quarto to create and share our work in this course; this website is also built using Quarto.||\nFollow the documentation to install Quarto. Be sure to ensure that you have the right version for your operating system.\n\nInstall the Quarto Extension for VS Code\nUnder “Step 2”, click on the VS Code icon.\n\n\nInstall Jupyter\nUnder the hood, Quarto uses Jupyter to run code. You don’t need to know how Jupyter works or worry about it, because it runs under the hood, but we will need to install it. Jupyter is a Python package.\nIf you don’t have Python installed (if you’re not sure, you should install Miniconda below), you’ll need to install it. The best way, by far, is to install Miniconda (see Conda documentation).\nOnce you have Python installed, you can open your Terminal (open VS Code then open the terminal), then run\npython3 -m pip install jupyter\n\n\n\n\n\n\nBased on past experience, getting Jupyter installed and Quarto to find your Jupyter installation is the most common source of problems, especially on Windows machines. Please start this early so you have a full week to get help if you need it."
  },
  {
    "objectID": "labs/lab-01/index.html#lab-instructions",
    "href": "labs/lab-01/index.html#lab-instructions",
    "title": "Lab 1: Software Installation",
    "section": "Lab Instructions",
    "text": "Lab Instructions\nOnce you have everything set up, your lab for the week will be to complete the following steps. These will ensure that your installation is working smoothly!\n\nFollow the link to lab 1 assignment from Canvas (it should start with classroom.github.com). You will get a message saying ” Your assignment repository has been created: …“. Click on the link to go to your repository.\nclone the repository for lab 01 (use the Github Classroom link from Canvas) to your computer. You can use VS Code functionality, GitHub Desktop, or your terminal.\nOpen the directory containing the repository in VS Code doing one of the following:\n\nFrom GitHub desktop: Repository &gt; Open in Visual Studio Code\nIn VS Code: File &gt; Open Folder...\n\nOpen the index.qmd file in VS Code and replace the author: CEVE 421/521 line with your name and netID\nOpen the JULIA Repl\n\nOpen the command palette (Ctrl+Shift+P on Windows/Linux, Cmd+Shift+P on Mac)\nStart typing “Julia: Start REPL”. It will auto-complete; select the command as it appears.\n\nSet up your project environment.\n\nIn the Julia REPL, type ] to enter the package manager. It should now show something like (lab-01) pkg&gt;.\nType instantiate and run it (Enter). This will install all the packages needed for this lab.\nType the backspace key to exit the package manager.\n\nEdit the index.qmd file to add your name and netID\nRender the document\n\nOpen the index.qmd file\nOpen the command palette and run “Quarto: Preview”. After some activity, a preview of the rendered document should open in VS Code. If you see something like Browse at http://localhost:4200/index.html you can open that link in your web browser to see the rendered document.\nRender to PDF or Microsoft Word following the directions below.\n\nIf you’re still having trouble:\n\nTry running build IJulia in the Julia REPL’s Pkg mode (type ])\nCome to office hours\nPost on the Canvas discussion for Lab 1\n\ncommit and push your changes to GitHub\nSubmit your rendered .docx or .pdf file to Canvas\n\n\nRunning Code\nWe can use Quarto to run Julia code in-line\n\nprintln(\"I'm using Julia!\")\n\nI'm using Julia!\n\n\nWe can also load packages\n\nusing CairoMakie\nusing LaTeXStrings\n\nand use them to make plots\n\nf = Figure()\nax = Axis(f[1, 1]; title=L\"$y = \\sin(x)$\", xlabel=L\"$x$\", ylabel=L\"$y$\")\nx = range(0, 10; length=100)\ny = sin.(x)\nlines!(ax, x, y)\nf\n\n\n\n\n\n\n\n\n\n\nRendering the Document\nA Quarto document is a plain text file that uses a simple code language to embed text, math, code, and the code’s output. You can run the document line by line, or you can render to a variety of formats. This is helpful for sharing with others, or for creating a final document for yourself.\nWe will check our ability to\n\nPreview the document in HTML (a web-native format)\nRender the document to PDF or Microsoft Word (a portable, shareable, and printable format)\n\n\nHTML\nFirst, verify that you can preview the document in HTML:\n\nOpen the command palette (Cmd+Shift+P on macOS, Ctrl+Shift+P on Windows/Linux)\nType “Quarto: Preview”\n\n\n\nPDF\nYou can use Quarto to generate PDF documents. Follow the instructions on Quarto’s website to install the necessary software.\n\nTo instruct Quarto to render to PDF, you need to edit the document metadata. Specifically, uncomment the lines in the document metadata corresponding to the PDF format.\nOpen the command palette (Cmd+Shift+P on macOS, Ctrl+Shift+P on Windows/Linux)\nType “Quarto: Render”\n\n\n\nWord\nYou can also render the document to Microsoft Word. See the Quarto documentation.\n\nTo instruct Quarto to render to Word, you need to edit the document metadata. Specifically, uncomment the lines in the document metadata corresponding to the Word format.\nOpen the command palette (Cmd+Shift+P on macOS, Ctrl+Shift+P on Windows/Linux)\nType “Quarto: Render”"
  },
  {
    "objectID": "labs/lab-06/template.html",
    "href": "labs/lab-06/template.html",
    "title": "Lab 6: Policy Search",
    "section": "",
    "text": "using Revise\nusing HouseElevation\n\nusing CSV\nusing DataFrames\nusing DataFramesMeta\nusing Distributions\nusing LaTeXStrings\nusing Metaheuristics\nusing Plots\nusing Random\nusing Unitful\n\nPlots.default(; margin=5Plots.mm)\n\n\nfunction objective_function(a::AbstractFloat)\n    return true # PLACEHOLDER\nend\n\nobjective_function (generic function with 1 method)"
  },
  {
    "objectID": "labs/lab-03/index.html",
    "href": "labs/lab-03/index.html",
    "title": "Lab 3: Depth-Damage Models",
    "section": "",
    "text": "Today, we’re going to be working with depth-damage functions. This will give us practice:\n\nworking with and manipulating tabular data\nwriting functions\n\nIn addition, the depth-damage function you choose / build will be a building block for your final project.\n\n\nAs before:\n\nClone the repository for this lab to your computer and open it in VS Code.\nIn the Julia REPL, activate and then instantiate the project environment.\nCheck that you can preview the project by running quarto preview template.qmd in the terminal (not Julia REPL). If that doesn’t work, open the Julia REPL, enter package mode with ], and run build IJulia.\nIf that doesn’t work, ask for help! The way VS Code looks for Python on your computer can be weird and counterintuitive.\n\n\n\n\nAs usual, we load all required packages at the top of the notebook, in one place.\n\nusing CSV\nusing DataFrames\nusing DataFramesMeta\nusing Distributions\nusing Interpolations\nusing Plots\nusing StatsPlots\nusing Unitful\n\n1Plots.default(; margin=6Plots.mm)\n\n\n1\n\nThis updates the default margin in our plots so that axis labels don’t get cut off."
  },
  {
    "objectID": "labs/lab-03/index.html#setup",
    "href": "labs/lab-03/index.html#setup",
    "title": "Lab 3: Depth-Damage Models",
    "section": "",
    "text": "As before:\n\nClone the repository for this lab to your computer and open it in VS Code.\nIn the Julia REPL, activate and then instantiate the project environment.\nCheck that you can preview the project by running quarto preview template.qmd in the terminal (not Julia REPL). If that doesn’t work, open the Julia REPL, enter package mode with ], and run build IJulia.\nIf that doesn’t work, ask for help! The way VS Code looks for Python on your computer can be weird and counterintuitive."
  },
  {
    "objectID": "labs/lab-03/index.html#load-packages",
    "href": "labs/lab-03/index.html#load-packages",
    "title": "Lab 3: Depth-Damage Models",
    "section": "",
    "text": "As usual, we load all required packages at the top of the notebook, in one place.\n\nusing CSV\nusing DataFrames\nusing DataFramesMeta\nusing Distributions\nusing Interpolations\nusing Plots\nusing StatsPlots\nusing Unitful\n\n1Plots.default(; margin=6Plots.mm)\n\n\n1\n\nThis updates the default margin in our plots so that axis labels don’t get cut off."
  },
  {
    "objectID": "labs/lab-03/index.html#parsing",
    "href": "labs/lab-03/index.html#parsing",
    "title": "Lab 3: Depth-Damage Models",
    "section": "Parsing",
    "text": "Parsing\nWe’d like to be able to use the depth-damage functions in this file. However, the depths are stored in a somewhat annoying format (e.g., “ft04m” means -4 feet). To make life simple, I’ve created some functionality in the depthdamage.jl file that you can use. We can load it as follows:\n\ninclude(\"depthdamage.jl\")\n\nDepthDamageData\n\n\nThe main thing that we’ll use is called DepthDamageData. This is a data structure or type that stores the depth-damage data, as well as any relevant metadata. If you’ve created a class in a language like C++ or Python, it’s the same idea. I’ve also defined a constructor that takes in the row of a DataFrame and creates a DepthDamageData object, to make life easy.\nI’ll show you how to do this for an illustrative depth-damage function from the New Orleans USACE.\n\ndemo_row = @rsubset(\n    haz_fl_dept, :Description == \"one story, Contents, fresh water, short duration\"\n)[\n    1, :,\n]\ndd = DepthDamageData(demo_row)\n\nDepthDamageData(Quantity{Float64, 𝐋, Unitful.FreeUnits{(ft,), 𝐋, nothing}}[-4.0 ft, -3.0 ft, -2.0 ft, -1.0 ft, 0.0 ft, 1.0 ft, 2.0 ft, 3.0 ft, 4.0 ft, 5.0 ft  …  15.0 ft, 16.0 ft, 17.0 ft, 18.0 ft, 19.0 ft, 20.0 ft, 21.0 ft, 22.0 ft, 23.0 ft, 24.0 ft], [0.0, 0.0, 0.0, 0.0, 0.0, 42.0, 63.0, 82.0, 85.0, 91.0  …  91.0, 91.0, 91.0, 91.0, 91.0, 91.0, 91.0, 91.0, 91.0, 91.0], String7(\"RES1\"), \"57\", String31(\"USACE - New Orleans\"), \"one story, Contents, fresh water, short duration\", \"missing\")\n\n\nThis prints out a bunch of data. We can see that it has the following fields, which should broadly match with our DataFrame:\n\nfieldnames(typeof(dd))\n\n(:depths, :damages, :occupancy, :dmg_fn_id, :source, :description, :comment)"
  },
  {
    "objectID": "labs/lab-03/index.html#plotting",
    "href": "labs/lab-03/index.html#plotting",
    "title": "Lab 3: Depth-Damage Models",
    "section": "Plotting",
    "text": "Plotting\nNow that we’ve created a DepthDamageData object, we can plot it. When we plot things with units, the Unitful package (as long as we are using it) knows how to handle them.\n\nscatter(\n    dd.depths,\n    dd.damages;\n    xlabel=\"Flood Depth at House\",\n    ylabel=\"Damage (%)\",\n    label=\"$(dd.description) ($(dd.source))\",\n    legend=:bottomright,\n    size=(700, 500),\n)"
  },
  {
    "objectID": "labs/lab-03/index.html#interpolating",
    "href": "labs/lab-03/index.html#interpolating",
    "title": "Lab 3: Depth-Damage Models",
    "section": "Interpolating",
    "text": "Interpolating\nThis is great. However, what if we want to estimate damage between the points? We need a way to interpolate. We can do this using the Interpolations package!\n\n1itp = let\n2    depth_ft = ustrip.(u\"ft\", dd.depths)\n    damage_frac = dd.damages\n    Interpolations.LinearInterpolation(\n        depth_ft,\n        damage_frac;\n3        extrapolation_bc=Interpolations.Flat(),\n    )\nend\n\n\n1\n\nI really like these let...end blocks and use them quite a bit. The main thing to know is that all the variables defined inside the let block are only available inside the let block. Once we get to the end of the block, they vanish! This keeps us from defining tons of variables that get in each others’ way.\n\n2\n\nThe Interpolations package doesn’t take units on its input, so we convert the input (which can be of any length unit) to feet before passing it in. If our depths are in meters or millimeters, it won’t be a problem – the ustrip function will convert to feet and then turn them into scalars.\n\n3\n\nInterpolations requires us to specify how to extrapolate. We choose Flat(), meaning that anything below the lowest value in the table will be assumed to have the same damage as the lowest value in the table and anything above the highest value in the table will be assumed to have the same damage as the highest value in the table.\n\n\n\n\nNow we can use this interpolation function to estimate damage at any depth.\n\nlet\n1    dmg_fn(x) = itp(ustrip.(u\"ft\", x))\n2    dmg_fn.([3.1u\"ft\", 2.2u\"m\", 91.4u\"inch\"])\nend\n\n\n1\n\nConvert the input to feet\n\n2\n\nEstimate damage at 3.1 feet, 2.2 meters, and 91.4 inches\n\n\n\n\n3-element Vector{Float64}:\n 82.30000000000001\n 91.0\n 91.0"
  },
  {
    "objectID": "labs/lab-03/index.html#packaging",
    "href": "labs/lab-03/index.html#packaging",
    "title": "Lab 3: Depth-Damage Models",
    "section": "Packaging",
    "text": "Packaging\nTo make life simple, we can define a function that takes in some depths and some damages and returns a function that can be used to estimate damage at any depth.\n\nfunction get_depth_damage_function(\n    depth_train::Vector{&lt;:T}, dmg_train::Vector{&lt;:AbstractFloat}\n) where {T&lt;:Unitful.Length}\n\n    # interpolate\n    depth_ft = ustrip.(u\"ft\", depth_train)\n    interp_fn = Interpolations.LinearInterpolation(\n1        depth_ft,\n        dmg_train;\n2        extrapolation_bc=Interpolations.Flat(),\n    )\n\n    damage_fn = function (depth::T2) where {T2&lt;:Unitful.Length}\n3        return interp_fn(ustrip.(u\"ft\", depth))\n    end\n4    return damage_fn\nend\n\n\n1\n\nThe Interpolations package doesn’t take units on its input, so we convert the input (which can be of any length) to feet before passing it in. If our depths are in meters or millimeters, it won’t be a problem – the ustrip function will convert to feet and then turn them into scalars.\n\n2\n\nInterpolations requires us to specify how to extrapolate. We choose Flat(), meaning that anything below the lowest value in the table will be assumed to have the same damage as the lowest value in the table and anything above the highest value in the table will be assumed to have the same damage as the highest value in the table.\n\n3\n\nThis is a bit confusing. We are defining a function, inside of a function.\n\n4\n\nWe return the function that we just defined. So when we call this function, we get a function – we in turn need to call that function on something else.\n\n\n\n\nget_depth_damage_function (generic function with 1 method)\n\n\n\ndamage_fn = get_depth_damage_function(dd.depths, dd.damages)\n\n#16 (generic function with 1 method)\n\n\nNow damage_fn is a function. It takes in a depth, with some type of length unit defined using Unitful, and returns the damage in percent. We can use this to plot a depth-damage curve:\n\np = let\n1    depths = uconvert.(u\"ft\", (-7.0u\"ft\"):(1.0u\"inch\"):(30.0u\"ft\"))\n2    damages = damage_fn.(depths)\n    scatter(\n        depths,\n        damages;\n        xlabel=\"Flood Depth\",\n        ylabel=\"Damage (%)\",\n        label=\"$(dd.description) ($(dd.source))\",\n        legend=:bottomright,\n        size=(800, 400),\n        linewidth=2,\n    )\nend\np\n\n\n1\n\nWe create a vector of depths from -7 feet to 30 feet, in 1 inch increments. We use uconvert to convert the units to feet (by default, Unitful converts to meters when we add together length units).\n\n2\n\nOur damage_fn is defined to take in a single scalar. To make predictions about a Vector of depths, we use . to broadcast the function over the vector.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOf course, if we use plot instead of scatter, then we get a line plot which is automatically smooth."
  },
  {
    "objectID": "labs/lab-04/index.html",
    "href": "labs/lab-04/index.html",
    "title": "Lab 4: House Elevation NPV Analysis",
    "section": "",
    "text": "Today we’re going to explore net present value (NPV) analysis in the context of a semi-realistic case study of house elevation.\nIn the previous lab, you developed a depth-damage relationship for a coastal structure and assessed how you might adjust the probability distribution of flooding at a nearby gauge to account for the structure’s height relative to the gauge. Today, you’ll use the same structure to compare the costs and benefits of elevating the structure to reduce the risk of flooding."
  },
  {
    "objectID": "labs/lab-04/index.html#setup",
    "href": "labs/lab-04/index.html#setup",
    "title": "Lab 4: House Elevation NPV Analysis",
    "section": "Setup",
    "text": "Setup\nAs always:\n\nClone the lab repository to your computer\nOpen the lab repository in VS Code\nOpen the Julia REPL and activate, then instantiate, the lab environment\nMake sure you can render: quarto render template.qmd in the terminal.\n\nIf you run into issues, try running ] build IJulia in the Julia REPL (] enters the package manager).\nIf you still have issues, try opening up blankfile.py. That should trigger VS Code to give you the option to install the Python extension, which you should do. Then you should be able to open a menu in the bottom right of your screen to select which Python installation you want VS Code to use.\n\n\nWe begin by loading our packages\n\nusing CSV\nusing DataFrames\nusing DataFramesMeta\nusing Distributions\nusing Interpolations\nusing Plots\nusing StatsPlots\nusing Unitful\n\nPlots.default(; margin=6Plots.mm)\n\nWe also leverage functions defined in another file, as before\n\ninclude(\"depthdamage.jl\")"
  },
  {
    "objectID": "labs/lab-04/index.html#adding-some-math-in",
    "href": "labs/lab-04/index.html#adding-some-math-in",
    "title": "Lab 4: House Elevation NPV Analysis",
    "section": "Adding some math in",
    "text": "Adding some math in\nWe have been using the following notation. In each time step, we calculate \\(u_t(a, \\mathbf{s})\\), where \\(a\\) is the action (in this case, how high we choose to elevate), and \\(\\mathbf{s}\\) is the “state of the world.” As noted above, \\(u_t(a, \\mathbf{s}) = -c_\\textrm{constr}(a) - \\mathbb{E}[c_\\textrm{damage}(a, \\mathbf{s})]\\), where \\(c_\\textrm{constr}(a)\\) is the cost of elevating the house and \\(c_\\textrm{damage}(a, \\mathbf{s})\\) is the expected cost of flooding in state \\(\\mathbf{s}\\) after taking action \\(a\\).\nFor now, let’s define our “state of the world” to have two pieces of information: the probability distribution of flooding, expressed as a Generalized Extreme Value distribution, and the discount rate. For now, we’ll treat the depth-damage function and the cost of elevating the house as fixed, although we could consider uncertainty.\nNow let’s build some of these steps out with some code."
  },
  {
    "objectID": "labs/lab-04/index.html#depth-damage-function",
    "href": "labs/lab-04/index.html#depth-damage-function",
    "title": "Lab 4: House Elevation NPV Analysis",
    "section": "Depth-damage function",
    "text": "Depth-damage function\nIn the previous lab, we used existing data to build a depth-data function. Remember that the depth here is relative to the house, not to the gauge.\n\nhaz_fl_dept = CSV.read(\"data/haz_fl_dept.csv\", DataFrame) # read in the file\ndesc = \"one story, Contents, fresh water, short duration\"\nrow = @rsubset(haz_fl_dept, :Description == desc)[1, :] # select the row I want\ndd = DepthDamageData(row) # extract the depth-damage data\ndamage_fn = get_depth_damage_function(dd.depths, dd.damages) # get the depth-damage function\n\n#13 (generic function with 1 method)\n\n\nWe can plot this as before\n\n\nCode\np = let\n    depths = uconvert.(u\"ft\", (-7.0u\"ft\"):(1.0u\"inch\"):(30.0u\"ft\"))\n    damages = damage_fn.(depths)\n    scatter(\n        depths,\n        damages;\n        xlabel=\"Flood Depth\",\n        ylabel=\"Damage (%)\",\n        label=\"$(dd.description) ($(dd.source))\",\n        legend=:bottomright,\n        size=(800, 400),\n        linewidth=2,\n    )\nend\np"
  },
  {
    "objectID": "labs/lab-04/index.html#annual-expected-flood-damages",
    "href": "labs/lab-04/index.html#annual-expected-flood-damages",
    "title": "Lab 4: House Elevation NPV Analysis",
    "section": "Annual expected flood damages",
    "text": "Annual expected flood damages\nAs discussed above, today we’ll focus on calculating the annual expected cost of flooding. In the previous lab, we used a Monte Carlo approach to estimate the expected cost of flooding. We’ll repeat that today. Recall that your offset will be different!\n\ngauge_dist = GeneralizedExtremeValue(5, 1, 0.1) # hypothetical gauge distribution\noffset = 7.5 # hypothetical height from house to gauge\nhouse_dist = GeneralizedExtremeValue(gauge_dist.μ - offset, gauge_dist.σ, gauge_dist.ξ)\n\n1samples = rand(house_dist, 100_000) .* 1u\"ft\"\n2damages = damage_fn.(samples)\n3expected_damages_pct = mean(damages)\n\n\n1\n\nDraw 100,000 samples from the distribution of flood heights at the house and add units of feet.\n\n2\n\nCalculate the damages for each sample using our function.\n\n3\n\nCalculate the expected damages as the mean of the damages. This is the Monte Carlo strategy \\(\\int p(x) f(x) dx \\approx \\frac{1}{N} \\sum_{i=1}^N f(x)\\) which requires \\(x_i \\sim p(x)\\).\n\n\n\n\n4.206083434187441\n\n\n\n\n\n\n\n\nThere’s no magic reason why we need to use 100,000 samples. Although this runs extremely fast, you could use fewer samples if you wanted to. A good way to check that you have enough samples is to re-run the experiment a few different times, and then to make sure that your expected damages don’t change much from run to run. Even with 100,000 samples, I see a change of about 0.2% from run to run.\n\n\n\nThe damages we have calculated are expressed as a percentage of the value of the house (structure and contents, not land). To convert this to a dollar value, we need to know the value of the house. This is of course tricky to estimate, but let’s use an example value. For your analysis, use Zillow or Redfin or similar to get a sense of the value of a house in the area you’re considering. Make some assumption about the fraction of the value that corresponds to the house structure relative to the land.\n\nhouse_structure_value = 250_000\nexpected_damages_usd = house_structure_value * expected_damages_pct / 100\n\n10515.208585468603\n\n\nWe can treat this as the expected cost of flooding for this model."
  },
  {
    "objectID": "labs/lab-04/index.html#cost-of-elevating",
    "href": "labs/lab-04/index.html#cost-of-elevating",
    "title": "Lab 4: House Elevation NPV Analysis",
    "section": "Cost of elevating",
    "text": "Cost of elevating\nNext, we have the cost of elevating. We’ll use equations I’ve used before Doss-Gollin & Keller (2023). Essentially, we have a piecewise linear function that depends on the area of the house and how height we elevate.\n\nhouse_area = 1000u\"ft^2\"\n\nTo get the cost function, we use the get_elevation_cost_function() function, which is defined in the depthdamage.jl file. This function fits an interpolator to the data, which we want because we don’t want to have to re-fit the interpolator every time we want to calculate the cost of elevating the house.\n\nelevation_cost = get_elevation_cost_function() # gives us a fitted interpolator\n\nelevation_cost (generic function with 1 method)\n\n\nWe can visualize this function as follows\n\n\nCode\nheights = uconvert.(u\"ft\", (0u\"ft\"):(1u\"inch\"):(10u\"ft\")) # some heights we will consider\nplot(\n    heights,\n    elevation_cost.(heights, house_area);\n    xlabel=\"How High to Elevate\",\n    ylabel=\"Cost (USD)\",\n    label=\"$(house_area)\",\n    tiitle=\"Cost of Elevating a House\",\n)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOf course, this simple approach is masking lots of important characteristics of each house and region that affect how expensive it might be to elevate."
  },
  {
    "objectID": "labs/lab-04/index.html#npv-analysis",
    "href": "labs/lab-04/index.html#npv-analysis",
    "title": "Lab 4: House Elevation NPV Analysis",
    "section": "NPV analysis",
    "text": "NPV analysis\nWe can use the functions above to calculate the NPV of elevating the house for a single year. However, if we are evaluating the house over a \\(T\\) year design window (recall: cost-benefit analysis needs a specific time horizon over which costs and benefits are computed) then we need to discount the costs and benefits to the present. We do this as \\[\n\\mathrm{NPV} = \\sum_{i=1}^T u_t(a, \\mathbf{s}) (1 - r) ^ {i-1}\n\\]\nLet’s say we have a 10 year design window and a discount rate of 5%. Let’s say we elevate zero feet. In that case, the cost of elevating is zero, and the expected cost of flooding is expected_damages_usd every year (neglecting any sea-level rise). Then we can calculate the NPV as follows.\nThen we can calculate the NPV as follows:\n\nannual_damages = [expected_damages_usd for _ in 1:10] # annual expected damages\ndiscount_rate = 0.05\nnpv = sum(annual_damages .* (1 - discount_rate) .^ (0:9))\n\n84387.29563104014\n\n\nanother, more concise, way to write this is\n\nnpv2 = sum([expected_damages_usd * (1 - discount_rate)^(i - 1) for i in 1:10])\n\n84387.29563104014\n\n\nboth are equivalent; you can use the one you prefer."
  },
  {
    "objectID": "labs/lab-04/index.html#single-year-function",
    "href": "labs/lab-04/index.html#single-year-function",
    "title": "Lab 4: House Elevation NPV Analysis",
    "section": "Single Year Function",
    "text": "Single Year Function\nFirst, we’re going to write a function that tells us our costs and benefits in a single year. The information we’ll need for that year is:\n\nThe distribution of of flooding at the house\nThe depth-damage function (as in percentage terms)\nThe cost of elevating the house (and the house area)\nThe house value\nHow high we elevated the house in that year.\n\nThis will look something like this\nfunction single_year_cost_benefit(flood_dist, damage_fn, elevation_cost, house_area, house_value, Δh)\n    \n    # calculate the expected damages\n    c_dmg = ...\n\n    # calculate the cost of elevating\n    c_constr = ...\n\n    # return the total cost and benefit\n    return -c_constr - c_dmg\nend"
  },
  {
    "objectID": "labs/lab-04/index.html#npv-function",
    "href": "labs/lab-04/index.html#npv-function",
    "title": "Lab 4: House Elevation NPV Analysis",
    "section": "NPV Function",
    "text": "NPV Function\nNext, we need to write a function that calculates the NPV over a \\(T\\) year design window. This function will take in all the information needed for the single_year_cost_benefit function, as well as the number of years T and the discount rate. Then, it will call the single_year_cost_benefit function for each year, and discount the costs and benefits to the present. Be sure to set \\(\\Delta h\\) to zero feet (you’ll get an error without units) for every year after the first!\nfunction npv_cost_benefit(flood_dist, damage_fn, elevation_cost, house_area, house_value, Δh, T, discount_rate)\n    # calculate the costs and benefits for each year, and then discount\n    # see above!\n    return npv\nend"
  },
  {
    "objectID": "labs/lab-04/index.html#one-sow-several-actions",
    "href": "labs/lab-04/index.html#one-sow-several-actions",
    "title": "Lab 4: House Elevation NPV Analysis",
    "section": "One SOW, several actions",
    "text": "One SOW, several actions\nFirst, let’s calculate the NPV for a single state of the world and two actions. Now that you have the npv_cost_benefit function, this should be straightforward. Guess how high you might want to elevate the house, and then calculate the NPV for that action.\nCompare your elevation to zero feet, and explore a few other elevations. What do you notice?"
  },
  {
    "objectID": "labs/lab-04/index.html#sensitivity-test",
    "href": "labs/lab-04/index.html#sensitivity-test",
    "title": "Lab 4: House Elevation NPV Analysis",
    "section": "Sensitivity test",
    "text": "Sensitivity test\nNow let’s perform a simple sensitivity test. Let’s assume that the discount rate is uncertain, as we explored in class. Use a Monte Carlo approach to estimate the expected NPV for a range of discount rates. As an example, you could use Normal(4, 2), which has a mean of 4 and a standard deviation of 2."
  },
  {
    "objectID": "labs/lab-04/index.html#discussion",
    "href": "labs/lab-04/index.html#discussion",
    "title": "Lab 4: House Elevation NPV Analysis",
    "section": "Discussion",
    "text": "Discussion\n\nWhat do you notice about the NPV for different actions?\nWhat do you notice about the sensitivity test?\nWhat are some limitations of this analysis?\n\nWhat things are missing from this analysis that you think are important?\nHow might they affect the results?\nWhat are some ways you might address these limitations?"
  },
  {
    "objectID": "labs/lab-05/index.html",
    "href": "labs/lab-05/index.html",
    "title": "Lab 5: Sea-Level Rise",
    "section": "",
    "text": "There are two objectives of this lab:"
  },
  {
    "objectID": "labs/lab-05/index.html#the-usual",
    "href": "labs/lab-05/index.html#the-usual",
    "title": "Lab 5: Sea-Level Rise",
    "section": "The usual",
    "text": "The usual\nAs always:\n\nClone the lab repository to your computer\nOpen the lab repository in VS Code\nOpen the Julia REPL and activate, then instantiate, the lab environment\nMake sure you can render: quarto render template.qmd in the terminal.\n\nIf you run into issues, try running ] build IJulia in the Julia REPL (] enters the package manager).\nIf you still have issues, try opening up blankfile.py. That should trigger VS Code to give you the option to install the Python extension, which you should do. Then you should be able to open a menu in the bottom right of your screen to select which Python installation you want VS Code to use."
  },
  {
    "objectID": "labs/lab-05/index.html#load-packages",
    "href": "labs/lab-05/index.html#load-packages",
    "title": "Lab 5: Sea-Level Rise",
    "section": "Load packages",
    "text": "Load packages\n\nusing CSV\nusing DataFrames\nusing DataFramesMeta\nusing Distributions\nusing Plots\nusing StatsPlots\nusing Unitful\n\nPlots.default(; margin=5Plots.mm)"
  },
  {
    "objectID": "labs/lab-05/index.html#local-package",
    "href": "labs/lab-05/index.html#local-package",
    "title": "Lab 5: Sea-Level Rise",
    "section": "Local package",
    "text": "Local package\nWe’re starting to accumulate a lot of code describing our model. A good way to store this model is by creating a local package. I have created a package called HouseElevation that contains the model code. You don’t need to do anything special to install it, and you don’t need to edit the code, though I’d encourage you to have a look around!\nWhen we work with local packages, it’s common to use another package called Revise. This is a cool package that will automatically propagate any changes you make to the package to any code that uses the package. You don’t need to worry about this for now – just load them.\n\nusing Revise\nusing HouseElevation"
  },
  {
    "objectID": "labs/lab-05/index.html#house",
    "href": "labs/lab-05/index.html#house",
    "title": "Lab 5: Sea-Level Rise",
    "section": "House",
    "text": "House\n\n\n\n\n\n\nWe will consider a single house, and will ignore uncertainty in the depth-damage function or other house parameters\n\n\n\n\nNeglect uncertainty in depth-damage function\nConsider a single building\nWe’re going to put all relevant information into a House object:\n\nDepth-damage function\nArea\nCost (USD)\nElevation relative to gauge\nMetadata\n\n\nWe can create a House as follows – note that we’re using a let...end block to create the House object. This means that any variables defined inside the block are not available outside the block, which is a good way to avoid “polluting the global namespace.”\n\nhouse = let\n    haz_fl_dept = CSV.read(\"data/haz_fl_dept.csv\", DataFrame) # read in the file\n    desc = \"one story, Contents, fresh water, short duration\"\n    row = @rsubset(haz_fl_dept, :Description == desc)[1, :] # select the row I want\n    area = 500u\"ft^2\"\n    height_above_gauge = 10u\"ft\"\n    House(\n        row;\n        area=area,\n        height_above_gauge=height_above_gauge,\n        value_usd=250_000,\n    )\nend\n\nWe can then use the House object to calculate the damage to the house for a given flood depth. Let’s convert the damage to dollars by multiplying the fraction (given by our depth-damage function) by the value of the house. For example:\n\n\nCode\nlet\n    depths = uconvert.(u\"ft\", (-7.0u\"ft\"):(1.0u\"inch\"):(30.0u\"ft\"))\n    damages = house.ddf.(depths) ./ 100\n    damages_1000_usd = damages .* house.value_usd ./ 1000\n    scatter(\n        depths,\n        damages_1000_usd;\n        xlabel=\"Flood Depth\",\n        ylabel=\"Damage (Thousand USD)\",\n        label=\"$(house.description)\\n($(house.source))\",\n        legend=:bottomright,\n        size=(800, 400),\n        yformatter=:plain, # prevents scientific notation\n    )\nend\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also use the House object to calculate the cost of raising the house to a given elevation. We use the elevation_cost function like this:\n\nelevation_cost(house, 10u\"ft\")\n\n67620.0\n\n\nand again we can plot this.\n\nlet\n    elevations = 0u\"ft\":0.25u\"ft\":14u\"ft\"\n    costs = [elevation_cost(house, eᵢ) for eᵢ in elevations]\n    scatter(\n        elevations,\n        costs ./ 1_000;\n        xlabel=\"Elevation\",\n        ylabel=\"Cost (Thousand USD)\",\n        label=\"$(house.description)\\n($(house.source))\",\n        legend=:bottomright,\n        size=(800, 400),\n        yformatter=:plain, # prevents scientific notation\n    )\nend"
  },
  {
    "objectID": "labs/lab-05/index.html#sea-level-rise",
    "href": "labs/lab-05/index.html#sea-level-rise",
    "title": "Lab 5: Sea-Level Rise",
    "section": "Sea-level rise",
    "text": "Sea-level rise\n\n\n\n\n\n\nWe will sample many different scenarios of sea-level rise\n\n\n\nWe’re modeling sea-level rise following the approach of Oddo et al. (2017). Essentially, we use five parameters: \\(a\\), \\(b\\), \\(c\\), \\(t^*\\), and \\(c^*\\). The local sea-level in year \\(t\\) is given by equation 6 of Oddo et al. (2017):\n\\[\n\\mathrm{SLR}= a + b(t - 2000) + c (t - 2000)^2 + c^* \\, \\mathbb{I} (t &gt; t^*) (t - t^*)\n\\]\nThe authors note:\n\nIn this model, the parameters \\(a\\), \\(b\\), and \\(c\\) represent the reasonably well-characterized process of thermosteric expansion as a second-order polynomial. It also accounts for more poorly understood processes, including potential abrupt sealevel rise consistent with sudden changes in ice flow dynamics.Here, \\(c^*\\) represents an increase in the rate of sea-level rise that takes place at some uncertain time, \\(t^*\\), in the future.\n\nThis is, of course, a highly simplified model. However, the parameters can be calibrated to match historical sea-level rise (i.e., throwing out any parameter values that don’t match the historical record) and use a statistical inversion method to estimate the parameters. One could also calibrate the parameters to match other, more complex, physics-based models. We’ll use Monte Carlo simulations from Oddo et al. (2017), available on GitHub. These were actually calibrated for the Netherlands, but we’ll pretend that sea-level rise in your location matches (which – as we know – it doesn’t).\n\nslr_scenarios = let\n    df = CSV.read(\"data/slr_oddo.csv\", DataFrame)\n    [Oddo17SLR(a, b, c, tstar, cstar) for (a, b, c, tstar, cstar) in eachrow(df)]\nend\nprintln(\"There are $(length(slr_scenarios)) parameter sets\")\n\nWe can plot these scenarios to get a sense of the range of sea-level rise we might expect.\n\nlet\n    years = 1900:2150\n    p = plot(;\n        xlabel=\"Year\",\n        ylabel=\"Mean sea-level (ft)\\nwith respect to the year 2000\",\n        label=\"Oddo et al. (2017)\",\n        legend=false\n    )\n    for s in rand(slr_scenarios, 250)\n        plot!(p, years, s.(years); color=:lightgrey, alpha=0.5, linewidth=0.5)\n    end\n    p\nend\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe key insight you should take from this plot is that uncertainty in future sea level increases over time!"
  },
  {
    "objectID": "labs/lab-05/index.html#storm-surge",
    "href": "labs/lab-05/index.html#storm-surge",
    "title": "Lab 5: Sea-Level Rise",
    "section": "Storm surge",
    "text": "Storm surge\n\n\n\n\n\n\nWe will consider parametric uncertainty in the storm surge\n\n\n\nThe next component of the model is the storm surge (i.e., the height of the flood above mean sea-level). We can model the water level at the gauge as the sum of the local sea-level and the storm surge. We can then model the water level at the house as the water level at the gauge minus the elevation of the house above the gauge.\nWe will consider parametric uncertainty in the storm surge. From lab 3, you should have a GeneralizedExtremeValue distribution for the storm surge. We can then sample parameters from a range centered on this distribution. For example, in the example for lab 3 we had GeneralizedExtremeValue(5, 1.5, 0.1). We can use this function to create a distribution for the storm surge.\n\nfunction draw_surge_distribution()\n    μ = rand(Normal(5, 1))\n    σ = rand(Exponential(1.5))\n    ξ = rand(Normal(0.1, 0.05))\n    GeneralizedExtremeValue(μ, σ, ξ)\nend\n\ndraw_surge_distribution (generic function with 1 method)\n\n\nWe can then call this function many times to get many different distributions for the storm surge. For example,\n[draw_surge_distribution() for _ in 1:1000]\n\n\n\n\n\n\nImportant\n\n\n\nThis is NOT statistical estimation. We are not saying anything at all about whether these parameters are consistent with observations. In fact, even when parameters are uncertain, sampling around a point estimate in this manner usually produces lots of parameter values that are highly implausible. Here, we are just exploring the implications of different parameter values. Building a better model for storm surge is a great idea for your final project!"
  },
  {
    "objectID": "labs/lab-05/index.html#discount-rate",
    "href": "labs/lab-05/index.html#discount-rate",
    "title": "Lab 5: Sea-Level Rise",
    "section": "Discount rate",
    "text": "Discount rate\n\n\n\n\n\n\nWe will consider parametric uncertainty in the discount rate.\n\n\n\nThe discount rate is an important economic parameter in our NPV analysis. There are elements of discounting that are perhaps not random (e.g., how much do you value the future versus the present?) while there are other elements that are very much random (what is the opportunity cost of spending money now?) We will model this by treating the discount rate as a random variable, but more sophisticated analyses are possible. We can use the following function\n\nfunction draw_discount_rate()\n    return rand(Normal(0.04, 0.02))\nend\n\nNote that we are now defining the discount rate as a proportion (from 0 to 1) rather than a percentage (from 0 to 100)."
  },
  {
    "objectID": "labs/lab-05/index.html#running-a-simulation",
    "href": "labs/lab-05/index.html#running-a-simulation",
    "title": "Lab 5: Sea-Level Rise",
    "section": "Running a simulation",
    "text": "Running a simulation\nIn the notation we’ve seen in class, we have a system model \\(f\\) that takes in a state of the world \\(\\mathbf{s}\\), an action \\(a\\), and outputs some metric or metrics. I’ve reproduced this in our model, adding one extra piece: a ModelParams object that contains all the parameters of the model that don’t change from one simulation to the next.\nIn our model, the ModelParams are the house characteristics (area, value, and depth-damage curve) and the years we’re considering. You should consider different time horizons!\n\np = ModelParams(\n    house=house,\n    years=2024:2083\n)\n\nThe next step is to create an object to hold our state of the world (SOW). We can create one like this. In the next step, we’ll want to sample a large ensemble of SOWs.\n\nsow = SOW(\n    rand(slr_scenarios),\n    draw_surge_distribution(),\n    draw_discount_rate()\n)\n\nLast, we need to define our action. For now, our action is very simple: we’re going to raise the house to a fixed elevation. However, in the future we might have a more complex action (e.g., when the sea level exceeds some threshold \\(t1\\), raise the house by some fixed amount \\(t2\\), which has two parameters). We define our action as follows:\n\na = Action(3.0u\"ft\")\n\nFinally, we have a function to run the simulation. This function takes in the model parameters, the state of the world, and the action, and returns the NPV of the action. Please have a look at run_sim.jl to see how this is implemented!\n\nres = run_sim(a, sow, p)\n\n-61567.0"
  },
  {
    "objectID": "labs/lab-05/index.html#apply-the-model-to-your-site",
    "href": "labs/lab-05/index.html#apply-the-model-to-your-site",
    "title": "Lab 5: Sea-Level Rise",
    "section": "Apply the model to your site",
    "text": "Apply the model to your site\n\nBuild your own house object, based on the house you’ve been using (or you can switch if you’d like)\n\nBriefly explain where you got the area, value, and depth-damage curve from\nPlot the depth-damage curve\nPlot the cost of raising the house to different elevations from 0 to 14 ft\n\nRead in the sea-level rise data\nModify my code to create a function to draw samples of storm surge and the discount rate. Explain your modeling choices!\nDefine an illustrative action, SOW, and model parameters, and run a simulation."
  },
  {
    "objectID": "labs/lab-05/index.html#large-ensemble",
    "href": "labs/lab-05/index.html#large-ensemble",
    "title": "Lab 5: Sea-Level Rise",
    "section": "Large ensemble",
    "text": "Large ensemble\nNow that you’ve got the model working for your site, you should run a large ensemble of simulations (explain how you interpret “large”).\n\nSample many SOWs (see below)\nSample a range of actions. You can do this randomly, or you can look at just a couple of actions (e.g., 0, 3, 6, 9, 12 ft) – explain your choice.\nRun the simulations for each SOW and action. You can use a for loop for this.\nCreate a DataFrame of your key inputs and results (see below)\n\nHere’s how you can create a few SOWs and actions and run the simulations for each:\n\nsows = [SOW(rand(slr_scenarios), draw_surge_distribution(), draw_discount_rate()) for _ in 1:10] # for 10 SOWs\nactions = [Action(3.0u\"ft\") for _ in 1:10] # these are all the same\nresults = [run_sim(a, s, p) for (a, s) in zip(actions, sows)]\n\n10-element Vector{Float64}:\n -201474.51163035625\n  -61569.36285870865\n      -1.423854944225773e6\n      -1.3021732643521933e6\n  -61626.03010265664\n  -87036.83302804582\n -928052.0364112327\n -116489.17482697271\n -353519.10354359075\n  -68903.25930691679\n\n\nHere’s how you can create a dataframe of your results. Each row corresponds to one simulation, and the columns are the inputs and outputs of the simulation.\n\ndf = DataFrame(\n    npv=results,\n    Δh_ft=[a.Δh_ft for a in actions],\n    slr_a=[s.slr.a for s in sows],\n    slr_b=[s.slr.b for s in sows],\n    slr_c=[s.slr.c for s in sows],\n    slr_tstar=[s.slr.tstar for s in sows],\n    slr_cstar=[s.slr.cstar for s in sows],\n    surge_μ=[s.surge_dist.μ for s in sows],\n    surge_σ=[s.surge_dist.σ for s in sows],\n    surge_ξ=[s.surge_dist.ξ for s in sows],\n    discount_rate=[s.discount_rate for s in sows],\n)\n\n\n10×11 DataFrame\n\n\n\nRow\nnpv\nΔh_ft\nslr_a\nslr_b\nslr_c\nslr_tstar\nslr_cstar\nsurge_μ\nsurge_σ\nsurge_ξ\ndiscount_rate\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n-2.01475e5\n3.0\n22.9774\n2.20796\n0.00236533\n2021.44\n11.9078\n3.40185\n2.61771\n0.108362\n0.0658803\n\n\n2\n-61569.4\n3.0\n23.5449\n1.94484\n0.00153052\n2053.67\n18.1674\n4.60407\n0.0863032\n0.194964\n0.0400582\n\n\n3\n-1.42385e6\n3.0\n57.8249\n3.39447\n0.0105961\n2086.08\n29.6286\n5.18299\n7.51906\n0.0619194\n0.0441824\n\n\n4\n-1.30217e6\n3.0\n23.2764\n2.09815\n0.00300118\n2036.4\n21.6261\n3.17178\n6.81534\n0.075341\n0.0350255\n\n\n5\n-61626.0\n3.0\n21.4145\n1.79808\n-0.000714661\n2018.12\n19.0127\n5.07453\n0.253032\n0.145868\n0.0467779\n\n\n6\n-87036.8\n3.0\n24.4432\n1.96953\n4.63e-5\n2056.52\n28.0188\n4.61363\n1.25201\n0.101843\n0.0387488\n\n\n7\n-928052.0\n3.0\n33.7914\n1.64173\n-0.00262429\n2053.13\n26.2433\n3.88874\n6.64865\n0.131573\n0.0564414\n\n\n8\n-1.16489e5\n3.0\n25.0673\n1.29723\n-0.00537052\n2065.56\n29.5369\n2.62028\n1.98159\n0.0686484\n0.0276271\n\n\n9\n-3.53519e5\n3.0\n20.0254\n1.68706\n-0.00121324\n2060.86\n31.9885\n3.96462\n3.10742\n0.109429\n0.0453946\n\n\n10\n-68903.3\n3.0\n37.7847\n2.39881\n0.00423055\n2048.96\n21.7618\n4.92152\n0.892895\n0.0549776\n0.0145667"
  },
  {
    "objectID": "labs/lab-05/index.html#analysis",
    "href": "labs/lab-05/index.html#analysis",
    "title": "Lab 5: Sea-Level Rise",
    "section": "Analysis",
    "text": "Analysis\nNow, analyze your results. You can use scatterplots and other visualizations, or any other statistical analyses that you think may be helpful. Remember that the goal is to understand how different parameter values affect the success or failure of different actions.\nSome questions to consider:\n\nWhen do you get the best results?\nWhen do you get the worst results?\nWhat are the most important parameters?\nIf you had unlimited computing power, would you run more simulations? How many?\nWhat are the implications of your results for decision-making?"
  },
  {
    "objectID": "labs/lab-02/index.html",
    "href": "labs/lab-02/index.html",
    "title": "Lab 2: Julia Quickstart",
    "section": "",
    "text": "In this lab we will learn how to work with tabular data in Julia. Specifically, you will get some experience using:\n\nDataFrames.jl to store tabular data as a DataFrame\nCSV.jl to read CSV files and convert them to DataFrames\nDataFramesMeta.jl to manipulate DataFrames\nPlots.jl and StatsPlots.jl to create visualizations\n\nFor those of you who took CEVE 543, you’ll find most of this familiar! If you find this challenging (e.g., if you’re new to programming) please look at the Resources page which has links to tutorials that you can use to get up to speed. Some of you will work extra hard this week, and some of you will have an easier week.\n\n\nIn this repository you will find two files.\n\nindex.qmd is the source code for the rendered document on the course website that you may be looking at now. That’s this file!\ntemplate.qmd is the template for your lab submission. You should edit it directly.\n\n\n\n\nAs with Lab 01, you should:\n\npush your final code to GitHub (I’ll be able to see it via GitHub classroom)\nsubmit your rendered PDF or DOCX file to Canvas"
  },
  {
    "objectID": "labs/lab-02/index.html#this-repository",
    "href": "labs/lab-02/index.html#this-repository",
    "title": "Lab 2: Julia Quickstart",
    "section": "",
    "text": "In this repository you will find two files.\n\nindex.qmd is the source code for the rendered document on the course website that you may be looking at now. That’s this file!\ntemplate.qmd is the template for your lab submission. You should edit it directly."
  },
  {
    "objectID": "labs/lab-02/index.html#submission",
    "href": "labs/lab-02/index.html#submission",
    "title": "Lab 2: Julia Quickstart",
    "section": "",
    "text": "As with Lab 01, you should:\n\npush your final code to GitHub (I’ll be able to see it via GitHub classroom)\nsubmit your rendered PDF or DOCX file to Canvas"
  },
  {
    "objectID": "labs/lab-02/index.html#clone-the-repository",
    "href": "labs/lab-02/index.html#clone-the-repository",
    "title": "Lab 2: Julia Quickstart",
    "section": "Clone the repository",
    "text": "Clone the repository\nFirst, you’ll need to clone this repository to your computer. As with Lab 01, I recommend to use GitHub Desktop or the built-in Git support in VS Code. Remember to use the link from Canvas (classroom.github.com/...).\nNext, open the repository in VS Code (you can do this directly from GitHub desktop if you’d like). All instructions from here assume you’re in the root directory of the repository."
  },
  {
    "objectID": "labs/lab-02/index.html#install-required-packages",
    "href": "labs/lab-02/index.html#install-required-packages",
    "title": "Lab 2: Julia Quickstart",
    "section": "Install required packages",
    "text": "Install required packages\nAs we saw in Lab 01, Julia is a modular language with code in packages. Compared to a language like Python, the packages in Julia typically have a narrower scope (for example, instead of a single Pandas package that does everything, there are separate packages for reading CSV files, defining dataframes, using clear syntax for data manipulation, etc.). When we’re working with a new lab, we’ll need to first install the packages we need.\n\nOpen the command palette and select Julia: Start REPL\nIn the Julia REPL, type ] to enter package manager mode\nType activate . to activate the project environment\nType instantiate to install the packages listed in the Project.toml file. This may take a few minutes.1"
  },
  {
    "objectID": "labs/lab-02/index.html#rendering-and-previewing",
    "href": "labs/lab-02/index.html#rendering-and-previewing",
    "title": "Lab 2: Julia Quickstart",
    "section": "Rendering and previewing",
    "text": "Rendering and previewing\nAs we saw in Lab 01, Quarto lets you convert a text file into an output. We’ve seen PDF, DOCX, and HTML (website) outputs, but there are other options as well.\nThere are many valid workflows, but my favorite is often to preview the document in VS Code while I’m working on it, and then render it when I’m done. To preview the document, run the following in your terminal (use the command palette in VS and then select “Terminal: Open a new terminal” or learn the shortcut keys):\nquarto preview template.qmd \nYou should see that a web browser opens up with the rendered document. As you make changes, they should appear in the browser automatically. You’ll see localhost:XXXX in your URL bar.\nTo render this document to PDF or DOCX, you have a few options. My favorite is to use the terminal again. For example, to convert to PDF:\nquarto render template.qmd --to pdf\nIf you run into issues, try the following two tips\n\nMake sure you’re typing these commands into the terminal and not into the Julia REPL\nIn the Julia REPL, type ] to enter package manager mode and then type build IJulia to rebuild your IJulia kernel (we won’t go into details)"
  },
  {
    "objectID": "labs/lab-02/index.html#getting-help",
    "href": "labs/lab-02/index.html#getting-help",
    "title": "Lab 2: Julia Quickstart",
    "section": "Getting help",
    "text": "Getting help\nIf you’re getting stuck, please:\n\nCome up and ask me questions if we’re in lab\nPost on Canvas discussions\nIf I can’t resolve your comment on Canvas, please email me to schedule a 1:1"
  },
  {
    "objectID": "labs/lab-02/index.html#looking-ahead",
    "href": "labs/lab-02/index.html#looking-ahead",
    "title": "Lab 2: Julia Quickstart",
    "section": "Looking ahead",
    "text": "Looking ahead\nIn the future, you’ll repeat these steps for every lab:\n\nclone the repository to your computer\nactivate the project environment\ninstantiate the packages\nmake your changes, saving and commiting regularly as you go\npush your changes to GitHub (you don’t have to wait until the end for this – you can push multiple times)"
  },
  {
    "objectID": "labs/lab-02/index.html#document-metadata",
    "href": "labs/lab-02/index.html#document-metadata",
    "title": "Lab 2: Julia Quickstart",
    "section": "Document metadata",
    "text": "Document metadata\nIf you open a Quarto file in your text editor (e.g., VS Code) or look at it on GitHub, you’ll see that the file starts with some metadata. The metadata is a set of key-value pairs that tell Quarto how to render the document. In Lab 01, you edited the author field to include your name."
  },
  {
    "objectID": "labs/lab-02/index.html#latex-math",
    "href": "labs/lab-02/index.html#latex-math",
    "title": "Lab 2: Julia Quickstart",
    "section": "LaTeX Math",
    "text": "LaTeX Math\nAs in standard Pandoc markdown, you can use LaTeX math in Quarto. For example, $\\alpha$ yields \\alpha. You can also use $$ to create a block equation:\n$$\nP(E) = \\{ n \\choose k \\} p ^k (2-p) ^ {n-k}\n$$\nrenders as\n\nP(E) = { n \\choose k } p ^k (2-p) ^ {n-k}\n\nFor more, see the “Typesetting Math” section of the resources page."
  },
  {
    "objectID": "labs/lab-02/index.html#source-code",
    "href": "labs/lab-02/index.html#source-code",
    "title": "Lab 2: Julia Quickstart",
    "section": "Source code",
    "text": "Source code\nSometimes we want to provide example code in our documents. This is code that is not meant to be run, but is just there to illustrate a point. We do that by wrapping the code in ```. For example:\n```\nf(x) = 1.25 * sin(2π * x / 1.5 + 0.5) + 0.25\nf(2.1)\n```\nyields\nf(x) = 1.25 * sin(2π * x / 1.5 + 0.5) + 0.25\nf(2.1)\nYou will typically want to specify the language of the code block, which will tell Quarto how to syntax highlight it. For example, see how the highlighting changes when we specify julia:\n```julia\nf(x) = 1.25 * sin(2π * x / 1.5 + 0.5) + 0.25\nf(2.1)\n```\nf(x) = 1.25 * sin(2π * x / 1.5 + 0.5) + 0.25\nf(2.1)"
  },
  {
    "objectID": "labs/lab-02/index.html#code-blocks",
    "href": "labs/lab-02/index.html#code-blocks",
    "title": "Lab 2: Julia Quickstart",
    "section": "Code blocks",
    "text": "Code blocks\nOften, we don’t just want to show code, but we want to run it and show the output.\n```{julia}\nf(x) = 1.25 * sin(2π * x / 1.5 + 0.5) + 0.25\nf(2.1)\n```\nwhich yields\n\nf(x) = 1.25 * sin(2π * x / 1.5 + 0.5) + 0.25\nf(2.1)\n\n0.4099583491000567\n\n\nYou can run these blocks in Julia by clicking the “Run Cell” button, or by pressing the keyboard shortcut (to see it, open the command palette and search for “Run Cell”). For more on Julia, see here."
  },
  {
    "objectID": "labs/lab-02/index.html#citations",
    "href": "labs/lab-02/index.html#citations",
    "title": "Lab 2: Julia Quickstart",
    "section": "Citations",
    "text": "Citations\nYou can add citations in Quarto. The easiest way is to export a bibliography from Zotero, and then add it to your Quarto document. You can use the Zotero Better BibTeX plugin to export a .bib file.\nSee here for instructions on using references with Quarto or see the website code for an example. I’ll provide a template for your final project."
  },
  {
    "objectID": "labs/lab-02/index.html#loading-packages",
    "href": "labs/lab-02/index.html#loading-packages",
    "title": "Lab 2: Julia Quickstart",
    "section": "Loading packages",
    "text": "Loading packages\nIn Julia we say using to import a package. By convention we’ll put these at the top of our script or notebook in alphabetical order. When you run this cell, you’ll see a bunch of activity in your REPL as Julia goes through the following steps:\n\nDownload a file from the internet that specifies which packages depend on which other packages\nSolve an optimization problem to identify which versions of which packages (including dependencies, and their dependencies, and so on) are compatible with each other\nDownload the packages and compile them (this may take a few minutes)\n\n\nusing CSV\nusing DataFrames\nusing DataFramesMeta\nusing Dates\nusing Plots\nusing StatsBase: mean\nusing StatsPlots\nusing Unitful"
  },
  {
    "objectID": "labs/lab-02/index.html#read-in-data",
    "href": "labs/lab-02/index.html#read-in-data",
    "title": "Lab 2: Julia Quickstart",
    "section": "Read in data",
    "text": "Read in data\nWe will use the CSV.jl package to read in our data.\n\n\n\n\n\n\nHover over the numbers on the right of this code for explanations.\n\n\n\n\n1fname = \"data/tidesandcurrents-8638610-1928-NAVD-GMT-metric.csv\"\n2df = CSV.read(fname, DataFrame)\n3first(df, 5)\n\n\n1\n\nWe define a variable called fname that stores the path to our data file. The data folder is in the same directory as this notebook.\n\n2\n\nWe use the CSV.read function to read in the data. The first argument is the filename, and the second argument tells Julia to convert the data to a DataFrame. We store it as a variable called df.\n\n3\n\nWe use the first function to show the first 5 rows of the DataFrame.\n\n\n\n\n\n5×5 DataFrame\n\n\n\nRow\nDate Time\nWater Level\nSigma\nI\nL\n\n\n\nString31\nFloat64\nFloat64\nInt64\nInt64\n\n\n\n\n1\n1928-01-01 00:00\n-0.547\n0.0\n0\n0\n\n\n2\n1928-01-01 01:00\n-0.699\n0.0\n0\n0\n\n\n3\n1928-01-01 02:00\n-0.73\n0.0\n0\n0\n\n\n4\n1928-01-01 03:00\n-0.669\n0.0\n0\n0\n\n\n5\n1928-01-01 04:00\n-0.516\n0.0\n0\n0\n\n\n\n\n\n\n\nThis data comes from the NOAA Tides and Currents website, specifically for a station at Sewells Point, VA for the year 1928. NAVD refers to the North American Vertical Datum, which is a reference point for measuring sea level, and GMT refers to Greenwich Mean Time, which is the time zone used in the data (rather than local time).\nWe can see that our DataFrame has five columns, the first of which is “Date Time”. However, the “Date Time” column is being parsed as a string. We want it to be a DateTime object from the Dates package. To do that, we need to tell Julia how the dates are formatted. We could then manually convert, but CSV.read has a kewyord argument that we can use\n\n1date_format = \"yyyy-mm-dd HH:MM\"\n2df = CSV.read(fname, DataFrame; dateformat=date_format)\nfirst(df, 3)\n\n\n1\n\nThis is a string that tells Julia how the dates are formatted. For example, 1928-01-01 00:00. See the documentation for more information.\n\n2\n\ndateformat is a keyword argument while date_format is a variable whose value is \"yyyy-mm-dd HH:MM\". We could equivalently write dateformat=\"yyyy-mm-dd HH:MM\".\n\n\n\n\n\n3×5 DataFrame\n\n\n\nRow\nDate Time\nWater Level\nSigma\nI\nL\n\n\n\nDateTime\nFloat64\nFloat64\nInt64\nInt64\n\n\n\n\n1\n1928-01-01T00:00:00\n-0.547\n0.0\n0\n0\n\n\n2\n1928-01-01T01:00:00\n-0.699\n0.0\n0\n0\n\n\n3\n1928-01-01T02:00:00\n-0.73\n0.0\n0\n0\n\n\n\n\n\n\n\nThe next column is “Water Level”, which is the height of the water above the reference point (NAVD) in meters. We can see that this is being parsed as a float, which is what we want 👍. However, you have to know that the data is in meters rather than inches or feet or something else. To explicitly add information about the units, we can use the Unitful package.\n\n1df[!, \" Water Level\"] .*= 1u\"m\"\nfirst(df, 3)\n\n\n1\n\nWe select the column with water levels using its name. The ! means “all rows”. Thus, df[!, \" Water Level\"] is a vector of all the water levels stored. *= means to multiply in place. For example, if x=2 then x *= 2 is equivalent to x = x * 2. .*= is a vector syntax, meaning do the multiplication to each element of the vector individually. 1u\"m\" is a Unitful object that represents 1 meter. We multiply the water levels by this to convert them to meters.\n\n\n\n\n\n3×5 DataFrame\n\n\n\nRow\nDate Time\nWater Level\nSigma\nI\nL\n\n\n\nDateTime\nQuantity…\nFloat64\nInt64\nInt64\n\n\n\n\n1\n1928-01-01T00:00:00\n-0.547 m\n0.0\n0\n0\n\n\n2\n1928-01-01T01:00:00\n-0.699 m\n0.0\n0\n0\n\n\n3\n1928-01-01T02:00:00\n-0.73 m\n0.0\n0\n0"
  },
  {
    "objectID": "labs/lab-02/index.html#subsetting-and-renaming",
    "href": "labs/lab-02/index.html#subsetting-and-renaming",
    "title": "Lab 2: Julia Quickstart",
    "section": "Subsetting and renaming",
    "text": "Subsetting and renaming\nWe want to only keep the first two (for more on the other three, see here). We can also rename the columns to make them easier to work with (spaces in variable names are annoying). To do this, we use the @rename function:\n\n1df = @rename(df, :datetime = $\"Date Time\", :lsl = $\" Water Level\");\n\n\n1\n\nThe $ is needed here because the right hand side is a string, not a Symbol.\n\n\n\n\nThen, we can use the @select function to select the columns we want. Notice how the first argument to select is the DataFrame and the subsequent arguments are column names. Notice also that our column names were strings (\"Date Time\"), but we can also use symbols (:datetime).\n\ndf = @select(df, :datetime, :lsl)\nfirst(df, 3)\n\n\n3×2 DataFrame\n\n\n\nRow\ndatetime\nlsl\n\n\n\nDateTime\nQuantity…\n\n\n\n\n1\n1928-01-01T00:00:00\n-0.547 m\n\n\n2\n1928-01-01T01:00:00\n-0.699 m\n\n\n3\n1928-01-01T02:00:00\n-0.73 m\n\n\n\n\n\n\n\nFor more on what DataFramesMeta can do, see this Tweet."
  },
  {
    "objectID": "labs/lab-02/index.html#time-series-plot",
    "href": "labs/lab-02/index.html#time-series-plot",
    "title": "Lab 2: Julia Quickstart",
    "section": "Time series plot",
    "text": "Time series plot\nNow we’re ready to make some plots of our data. Let’s start with a simple time series plot of the water levels. Our data is collected hourly, so we have a lot of data points! Still, we can plot them all.\n\nplot(\n    df.datetime,\n    df.lsl;\n    title=\"Hourly Water levels at Sewells Point, VA\",\n    ylabel=\"Water level\",\n    label=false,\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFocusing on the entire time series means we can’t dig into the details. Let’s zoom in on a single month (October 1928) using the @subset function.\n\n1t_start = Dates.DateTime(1928, 10, 1, 0)\nt_end = Dates.DateTime(1928, 10, 31, 23)\n2df_month = @subset(df, t_start .&lt;= :datetime .&lt;= t_end)\nfirst(df_month, 3)\n\n\n1\n\nThis creates a DateTime object for the start of October 1928 at 0 hours. Defining it clearly here aids readability.\n\n2\n\nThis selects all the rows where the :datetime column is between t_start and t_end. The . syntax is called dot broadcasting and is a way to apply a function to each element of a vector.\n\n\n\n\n\n3×2 DataFrame\n\n\n\nRow\ndatetime\nlsl\n\n\n\nDateTime\nQuantity…\n\n\n\n\n1\n1928-10-01T00:00:00\n0.215 m\n\n\n2\n1928-10-01T01:00:00\n0.429 m\n\n\n3\n1928-10-01T02:00:00\n0.581 m\n\n\n\n\n\n\n\nNow we can plot it as above:\n\nplot(\n    df_month.datetime,\n    df_month.lsl;\n    title=\"Water levels at Sewells Point, VA\",\n    ylabel=\"Water level\",\n    label=false,\n)"
  },
  {
    "objectID": "labs/lab-02/index.html#climatology",
    "href": "labs/lab-02/index.html#climatology",
    "title": "Lab 2: Julia Quickstart",
    "section": "Climatology",
    "text": "Climatology\nAn essential idea in working with tabular data (and other data formats) is “split-apply-combine”. Essentially: split the data into groups, apply some function to each group, and then combine the results.\nWe can use this workflow to answer an interesting question: what is the average water level for each month?2 Of course, we’re only looking at one year of data here – we should ideally look at a long record!\n\n1df[!, :month] = Dates.month.(df.datetime)\n2dropmissing!(df, :lsl)\n3df_bymonth = groupby(df, :month)\n4df_climatology = combine(df_bymonth, :lsl =&gt; mean =&gt; :lsl_avg);\n\n\n1\n\nThis creates a new column called :month that is the month of each observation.\n\n2\n\nThis will discard any rows in df that have a missing value of :lsl. This is necessary because the mean function will return missing if any of the values are missing.\n\n3\n\nThis creates a GroupedDataFrame object that contains all the data grouped by month.\n\n4\n\nThis takes the grouped data and calculates the mean of the :lsl column for each month. The general syntax is combine(grouped_df, :column =&gt; function).\n\n\n\n\nWe can now plot the climatology.\n\nplot(\n1    df_climatology.month,\n    df_climatology.lsl_avg;\n2    xticks=1:12,\n    xlabel=\"Month\",\n    ylabel=\"Average Water level\",\n3    linewidth=3,\n    label=false,\n)\n\n\n1\n\nWe can use df.colname instead of df[!, :colname]. The latter is more robust but the former is easier to type.\n\n2\n\nSetting xticks will set the x-axis ticks to the values in the vector. We can use this to make sure the x-axis ticks are labeled with the months.\n\n3\n\nWe can set the line width to make the plot easier to read."
  },
  {
    "objectID": "labs/lab-02/index.html#footnotes",
    "href": "labs/lab-02/index.html#footnotes",
    "title": "Lab 2: Julia Quickstart",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJulia precompiles packages when they are installed, and (to a lesser extent) when they are first used. The first time you use a package it may take a moment to load. This is normal, nothing to worry about, and rapidly improving.↩︎\nTo do a better job, we should separate out the long-term trend from the seasonal cycle. This is called de-trending and is a common technique in climate science. We can worry more about this later.↩︎"
  },
  {
    "objectID": "readings/week-09-reading.html",
    "href": "readings/week-09-reading.html",
    "title": "Readings for Week 9",
    "section": "",
    "text": "Please read from Quinn et al. (2017) and answer the following questions:\n\nWhat is being optimized in this paper?\nWhat uncertainties and processes are being considered? Are there any relevant considerations not included in the model?\nThe authors discussed that this work creates two new technological contributions: “(1) demonstrating the value and use of direct policy search (DPS) (Rosenstein & Barto, 2001) for identifying adaptive robust operational control strategies for socio-ecological systems and (2) demonstrating how nonlinear environmental thresholds, or tipping points, pose fundamental challenges for balancing economic benefits and their consequent risks to socio-ecological systems”. Why do you think these contributions are important?\nThis paper explores the modeling of “socio-ecological systems with tipping points,” specifically the classical shallow lake problem. Discuss other examples of these types of systems. Do you think climate change will produce similar systems?\n\n\n\n\n\nReferences\n\nQuinn, J. D., Reed, P. M., & Keller, K. (2017). Direct policy search for robust multi-objective management of deeply uncertain socio-ecological tipping points. Environmental Modelling & Software, 92, 125–141. https://doi.org/10.1016/j.envsoft.2017.02.017"
  },
  {
    "objectID": "readings/week-13-reading.html",
    "href": "readings/week-13-reading.html",
    "title": "Readings for Week 13",
    "section": "",
    "text": "Discussion questions will be posted here.\n\n\n\nAssigned reading will be drawn from:\n\nThomson et al. (2023)\nCondon (2021)\n\n\n\n\n\nReferences\n\nCondon, M. (2021). Market myopia’s climate bubble. https://doi.org/10.2139/ssrn.3782675\n\n\nThomson, H., Zeff, H. B., Kleiman, R., Sebastian, A., & Characklis, G. W. (2023). Systemic Financial Risk Arising From Residential Flood Losses. Earth’s Future, 11(4), e2022EF003206. https://doi.org/10.1029/2022EF003206"
  },
  {
    "objectID": "readings/week-03-reading.html",
    "href": "readings/week-03-reading.html",
    "title": "Readings for Week 3",
    "section": "",
    "text": "Please read Wing et al. (2020) for Wednesday’s class. As you read, please post any clarifying questions you have to Canvas.\nThis paper analyzes insurance claims data to evaluate and explore “depth-damage” curves. Please consider the following questions as you read:\n\nWhat is the difference between a probabilistic and deterministic depth-damage curve? What are practical advantages of a probabilistic depth-damage curve?\nIf you were predicting flood damages for a structure, what information besides flood depth might be useful?\nWhat might explain the regional patterns of depth-damage curves in the paper?\nWhat are some problems with using insurance claims data to develop or validate depth-damage curves?\n\nPlease also prepare a discussion question for class based on this paper.\n\n\n\n\nReferences\n\nWing, O. E. J., Pinter, N., Bates, P. D., & Kousky, C. (2020). New insights into US flood vulnerability revealed from flood insurance big data. Nature Communications, 11(1, 1), 1444. https://doi.org/10.1038/s41467-020-15264-2"
  },
  {
    "objectID": "readings/week-01-reading.html",
    "href": "readings/week-01-reading.html",
    "title": "Week 1 readings",
    "section": "",
    "text": "Please read the following news articles:\n\nFrank (2022)\nSommer (2022)\n\nYou are encouraged, though not required, to use Zotero to store your readings. You can save news articles using the browser extension for your browser of choice. If you save the news article as a PDF (go to “print” and then choose “save as PDF”) you can link the PDF to your library and your annotations will be saved.\nThere are other great tools, including Randrop.io and hypothes.is. Figure out what works for you!\n\n\n\n\nReferences\n\nFrank, T. (2022, August 22). Bold New Jersey Shore Flood Rules Could Be Blueprint for Entire U.S. Coast. Scientific American. Retrieved from https://www.scientificamerican.com/article/bold-new-jersey-shore-flood-rules-could-be-blueprint-for-entire-u-s-coast/\n\n\nSommer, L. (2022, February 9). An unexpected item is blocking cities’ climate change prep: Obsolete rainfall records. NPR: Climate. Retrieved from https://www.npr.org/2022/02/09/1078261183/an-unexpected-item-is-blocking-cities-climate-change-prep-obsolete-rainfall-reco"
  },
  {
    "objectID": "readings/week-14-reading.html",
    "href": "readings/week-14-reading.html",
    "title": "Readings for Week 14",
    "section": "",
    "text": "Discussion questions will be posted here.\n\n\n\nAssigned reading will be drawn from:\n\nKeller et al. (2021)\nGilligan & Vandenbergh (2020)\n\n\n\n\n\nReferences\n\nGilligan, J. M., & Vandenbergh, M. P. (2020). Beyond wickedness: Managing complex systems and climate change. Vanderbilt Law Review, 73, 1777–1810. Retrieved from https://wp0.vanderbilt.edu/lawreview/2020/12/beyond-wickedness-managing-complex-systems-and-climate-change/\n\n\nKeller, K., Helgeson, C., & Srikrishnan, V. (2021). Climate risk management. Annual Review of Earth and Planetary Sciences, 49(1), 95–116. https://doi.org/10.1146/annurev-earth-080320-055847"
  },
  {
    "objectID": "readings/week-07-reading.html",
    "href": "readings/week-07-reading.html",
    "title": "Readings for Week 7",
    "section": "",
    "text": "Please read Schwetschenau et al. (2023). This is a fairly technical paper that uses optimization techniques that I do not expect you to know or even be able to follow! Please focus on the introduction, discussion, and conclusions; look at the methods to understand what the authors are optimizing (what are the decision variables and what is the objective function), but don’t worry about the details of the optimization itself. Do not spend lots of time following the math unless you find it personally interesting!"
  },
  {
    "objectID": "readings/week-07-reading.html#discussion-questions",
    "href": "readings/week-07-reading.html#discussion-questions",
    "title": "Readings for Week 7",
    "section": "Discussion questions:",
    "text": "Discussion questions:\n\nWho is the intended audience of this paper?\nThis paper uses an exploratory model to suggest infrastructure and financial decisions. What might be some risks associated with this? What do the authors do to mitigate this risk?\nWhat problem is facing the community of UnionTown? How do the authors hope to address this problem?\nDo you think that the results, as presented, are useful from a policy making perspective? Why or why not?\nAre there any externalities or things that are being unaccounted for that we might also want to consider in this paper? If yes, what are they? -= (Optional bonus for anyone who’s done a bit of AI or wants to do a bit of extra research) The authors here use Dijkstra’s algorithm. Can you foresee any risks with applying that to this situation?"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Climate variability and change pose threats to lives and livelihoods. These climate risks can be managed through the design and operation of infrastructure systems, as well as through disaster response and recovery. Decisions about how to develop and choose risk management strategies are often based on pure vibes, but occasionally rigorous quantitative analyses that make use of scientific information can inform them (we will focus on these cases). These analyses involve integrating knowledge from multiple disciplines to balance competing goals (objectives) under uncertainty.\nIn this course, you will learn climate science, uncertainty quantification, and decision analysis methods to support climate risk management. You will be assigned readings for every class that cover both methods and applications, and will work collaboratively to implement key concepts through programming problem sets. Active class participation is required. Methods covered include scenario analysis, exploratory modeling, cost-benefit analysis, single- and multi-objective policy search, reinforcement learning, deep uncertainty, robust decision making, and equitable decision making.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "Climate variability and change pose threats to lives and livelihoods. These climate risks can be managed through the design and operation of infrastructure systems, as well as through disaster response and recovery. Decisions about how to develop and choose risk management strategies are often based on pure vibes, but occasionally rigorous quantitative analyses that make use of scientific information can inform them (we will focus on these cases). These analyses involve integrating knowledge from multiple disciplines to balance competing goals (objectives) under uncertainty.\nIn this course, you will learn climate science, uncertainty quantification, and decision analysis methods to support climate risk management. You will be assigned readings for every class that cover both methods and applications, and will work collaboratively to implement key concepts through programming problem sets. Active class participation is required. Methods covered include scenario analysis, exploratory modeling, cost-benefit analysis, single- and multi-objective policy search, reinforcement learning, deep uncertainty, robust decision making, and equitable decision making.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "Syllabus",
    "section": "Course Information",
    "text": "Course Information\n\n\n\nInstructor\n\n James Doss-Gollin\n jdossgollin@rice.edu\n Ryon 215\n\n\n\n\nTA\nTBD\n\n\n\nMeetings\n\n MWF\n 11-11:50am\n TBD\n\n\n\n\n\nPrerequisites & Preparation\nThe following prerequisites are required:\n\nAn introductory course in probability and statistics, such as CEVE 313, is strictly required.\n\nIn addition, the following prerequisites are encouraged.\n\nSome exposure to Python, Julia, Matlab, R, or another programming language\nAdditional coursework in applied statistics\nLinear algebra (you should be comfortable with matrix notation and basic operations)\nOptimization (you should be comfortable writing down optimization problems)\n\nIf you are unsure whether your background gives you an adequate preparation for this course, please contact the instructor!\n\n\n\n\n\n\nWhat If My Skills Are Rusty?\n\n\n\nIf your programming, mathematics, or statistics skills are a little rusty, don’t worry! We will review concepts and build skills over the course of the semester.\n\n\n\n\nCourse Objectives\nAfter completing this course, you will be able to:\n\nEvaluate and describe the strengths and weaknesses of different approaches to modeling the impact of weather and climate hazards on critical systems.\nApply and critique methods for cost-benefit analysis, optimization, policy search, and stochastic control to climate adaptation problems.\nDescribe multiple frameworks for decision making under deep uncertainty.\nDecision analytical frameworks well suited to a particular problem and justify the choice.\n\n\n\nRequired Materials\nNo textbook is required for this course. All materials will be posted as open source on the course website or on Canvas.\nYou will regularly be assigned scientific papers to read. Where those are available through the Rice library, you will be expected to access them yourself. You are encouraged, though not required, to use Zotero (Rice students have free storage). See Fondren Library’s Resources for resources.\nYou will need a working laptop for this class. If you do not have access to a working laptop, or if you lose access during the semester, please email the instructor.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#a-community-of-learning",
    "href": "syllabus.html#a-community-of-learning",
    "title": "Syllabus",
    "section": "A Community of Learning",
    "text": "A Community of Learning\nRice’s core values are responsibility, integrity, community, and excellence. Our goal is to create a learning community aligned with these core values.\n\nCore Expectations\nCourse success involves a dual responsibility on the part of the instructor and the student.\n\n\nAs the instructor, my responsibility is to provide you with a structure and opportunity to learn. To this end, I commit to:\n\nprovide organized and focused lectures, in-class activities, and assignments;\nencourage students to regularly evaluate and provide feedback on the course;\nmanage the classroom atmosphere to promote learning;\nschedule sufficient out-of-class contact opportunities, such as office hours;\nallow adequate time for assignment completion;\nmake lecture materials, class policies, activities, and assignments accessible to students.\n\n\n\n\nStudents are responsible for their own learning in the course and should commit to:\n\nattending all lectures;\ndoing all required preparatory work before class;\nactively participating in online and in-class discussions;\nbeginning assignments and other work early; and\nattending office hours as needed.\n\n\n\n\n\n\n\n\n\nWhat If I’m Sick?\n\n\n\nPlease stay home if you’re feeling sick! This is beneficial for both for your own recovery and the health and safety of your classmates. We will also make any necessary arrangements for you to stay on top of the class material and if whatever is going on will negatively impact your grade, for example by causing you to be unable to submit an assignment on time.\n\n\n\n\nDiversity, Equity, and Inclusion\nRice is committed to building and maintaining an equitable and inclusive campus community. Diversity can refer to multiple ways that we identify ourselves, including but not limited to race, color, national origin, language, sex, disability, age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status, or genetic information. Each of these diverse identities, along with many others not mentioned here, shape the perspectives our students, faculty, and staff bring to our campus. We, at Rice, will work to promote diversity, equity and inclusion not only because diversity fuels excellence and innovation, but because we want to pursue justice. We acknowledge our imperfections while we also fully commit to the work, inside and outside of our classrooms, of building and sustaining a campus community that increasingly embraces these core values.\nEach of us is responsible for creating a safer, more inclusive environment.\n\n\nAccommodation for Students with Disabilities\nIf you have a documented disability or other condition that may affect academic performance you should: 1) make sure this documentation is on file with the Disability Resource Center (Allen Center, Room 111 / adarice@rice.edu / x5841) to determine the accommodations you need; and 2) talk with me to discuss your accommodation needs.\n\n\nAccommodation for Scheduling Conflicts\nIf any of our class meetings conflict with your religious events, student athletics, or other non-negotiable scheduling conflict, please let me know ASAP so that we can make arrangements for you.\n\n\nMask Policies\nMasks are welcome but not required in the classroom. However, if a colleague (student, faculty, or staff) requests that others wear a mask, you are strongly encouraged to make them feel safe. Please do not ask someone making such a request to disclose their underlying medical condition. If for some reason you need your instructor or classmates to wear a mask, please let me know and I will communicate this to the class without disclosing your identity.\nThese policies may change over the course of the semester as the situation evolves.\n\n\nGetting Help\nYou can ask questions through Canvas Discussions and Office Hours. We will make an effort to respond to all Canvas discussion questions within 24 hours, though this won’t always be possible (travel, weekends, etc.). Please do not use email for questions about course content or labs, since other students may have related questions. You should use email for questions about personal matters, such as scheduling conflicts, accommodations, etc.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nThis class is designed to encourage collaboration, and students are encouraged to discuss their work with other students. Engineering as a profession relies upon the honesty and integrity of its practitioners (see e.g. the American Society for Civil Engineers’ Code of Ethics). All work submitted must represent the students’ own work and understanding, whether individually or as a group (depending on the particulars of the assignment).\nIf you are ever unclear about academic integrity, please ask! Additionally, always err on the side of providing more information.)\n\nRice Honor Code\nMore specifically, all students will be held to the standards of the Rice Honor Code, a code that you pledged to honor when you matriculated at this institution. If you are unfamiliar with the details of this code and how it is administered, you should consult the Honor System Handbook at honor.rice.edu/honor-system-handbook/. This handbook outlines the University’s expectations for the integrity of your academic work, the procedures for resolving alleged violations of those expectations, and the rights and responsibilities of students and faculty members throughout the process.\n\n\nAI/ML Resource Policy\nLarge language models (LLMs), like GPT, are powerful tools for generating text that can be used for coding and doing data analysis. This is at once empowering (LLMs are powerful and can save you time) and risky (LLMs can make mistakes that are hard to detect).\nOur general view is that LLMs are powerful tools that you will encounter and use when you leave this classroom, so it’s important to learn how to use them responsibly and effectively. You are generally permitted to use LLMs in this course, but ultimately, you are responsible for guaranteeing, understanding, and interpreting your results. In particular:\n\nOne of the best applications of LLMs is to write code. This can help accelerate your workflow, especially when you are learning new syntax. However, LLMs can make bad decisions about how to structure your code, can introduce bugs, and can mislead you about what your code is doing. You are responsible for understanding and debugging your code, and for ensuring that it does what you intend it to do.\nLLMs should not be used to generate text that you submit as your own work. If you are assigned a writing assignment, the point is to stimulate your thought process, and you short-cut this if you ask a LLM to generate the response for you. This leads to shallow thinking! However, you may use tools including LLMs (but also Grammarly, spell-check, etc.) to help you edit your writing. This can sometimes be a fine line; it’s always better to ask if you’re not sure, and to disclose your use of these tools in your submission\n\nThe resources page has links to helpful ideas about LLMs and how to use them.\n\n\nPolicy on Web Posting of Course Materials\nUploading course materials to web sites is not an authorized use of the course material. Both the poster and the user are in violation of the university policy, which is actionable.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nYour final grade will be calculated as follow:\n\n\n\nCategory\nPoints (421)\nPoints (521)\n\n\n\n\nReading quizzes\n10\n10\n\n\nLabs\n10\n10\n\n\nTests\n40\n30\n\n\nProject\n30\n30\n\n\nReading Discussion\n0\n10\n\n\nReading Notes\n10\n10\n\n\n\n\nReading Quizzes\nThe purpose of assigning readings is to enhance class discussions. Students who are prepared for class enhance the learning experience for everyone and enable a collaborative group discussion. At the start of classes for which reading was assigned, expect a very short (5 minute) quiz covering basic concepts from the reading. You should be able to get all points on these readings if you have done the reading, even if you found key concepts challenging or confusing.\nYour lowest two reading quizzes will be dropped.\n\n\nLabs\nOn Fridays we will use class time for hands-on programming exercises (“labs”) to give you guided practice applying the concepts and methods from class. These labs will be announced on the course website ahead of time so anyone who is able can bring a laptop to class. These labs can be done in groups; if you cannot bring a laptop to class for whatever reason, you will be able to (and are encouraged to) work with other students, though you must turn in your own assignment for grading. All labs will cover either a set of programming tools or a case study.\nYour lowest two labs will be dropped.\n\n\nTests\nIn-class written exams will be given for each module of the course. Tests are designed to be straightforward and will cover key ideas from reading, as well as key terms and concepts from lecture and code interpretation related to labs.\n\n\nProject\nWe will build a case study related to house elevation over the course of the semester through weekly labs. Answers will be posted for each lab, so mistakes made in one lab do not pass to the next.\nFor a final project, you will be asked to incorporate additional concept(s) from the course into the case study. For example, you might improve the representation of additional sources of uncertainty, use a more sophisticated optimization algorithm, or change the objectives of the optimization problem.\nStudents enrolled in the 421 section (i.e., undergraduates) may work in small groups. Students enrolled in the 521 section (i.e., graduate students) must work individually.\n\n\nReading Discussion\nAll students enrolled in the 521 section (i.e., graduate students) will present one of the papers assigned to the class and lead a discussion.\n\nThis discussion should take 30-50 minutes.\nYou should sign up for a time by the second week of class.\nYou should meet with your instructor at least one week before your presentation to discuss the reading and your discussion plan.\n\nYou will be graded on the quality of your presentation, the depth of your understanding of the reading, and your ability to lead a discussion.\nStudents enrolled in the 421 section may choose to present a paper alone or in a group. If you choose to present a paper, you will be graded on the same scale as the 521 students.\n\n\nReading Notes\nAll students will act as “note-taker” for an in-class reading discussion. The note-taker will be responsible for taking notes on the discussion (following up from the recording as necessary), then adding them to the course website (through a Pull Request; guidance will be provided). These notes should summarize, rather than transcribe, the discussion, and identify key insights, questions, disagreements, and points of confusion.\nYou should sign up for a time by the second week of class. Groups may be permitted depending on enrollment.\n\n\nLate Work Policy\nLate projects will be subjected to a 10% penalty per day. Specifically, your grade will be multiplied \\(0.9^d\\) where \\(d\\) is the number of days late. Late labs will not be accepted so that solutions can be posted promptly.\nAs described above, your lowest two labs and reading quizzes will be dropped. This is intended to accommodate events like minor illnesses, travel to a conference, and other circumstances that may cause you to miss a lab or quiz. If a major circumstance arises (e.g., a death in the family, a serious illness, etc.) that causes you to miss an extended period of time, please contact the instructor to discuss accommodations.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#preliminary-schedule",
    "href": "syllabus.html#preliminary-schedule",
    "title": "Syllabus",
    "section": "Preliminary Schedule",
    "text": "Preliminary Schedule\nThe following schedule outlines our planned topics and readings for the semester. In general, we will use Mondays for lecture, Wednesdays for group discussion centered on readings, and Fridays for computational labs.\nThis schedule is subject to change. Updated versions will be posted on the Schedule page of the course website.\n\n\n\n\n\n\nNot all readings listed here will be assigned in full. Some may be assigned as optional reading and others may be assigned as excerpts.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nCourse Dates\nTopic\nReading\nExam / Project\n\n\n\n\n\nModule 1:\nIntroduction\n\n\n\n\n1\n1/8, 1/10, 1/12\nIntroduction to climate risk management\nFrank (2022), Loucks (2017) Ch. 1\n\n\n\n2\n1/17, 1/19\nScience of climate hazard\nSeneviratne et al. (2021), Lall et al. (2018)\n\n\n\n3\n1/22, 1/24, 1/26\nVulnerability, exposure, and impacts\nWing et al. (2020), Bonnafous et al. (2017)\n\n\n\n4\n1/29, 1/31, 2/2\nSystems\nReed et al. (2022),\nExam 1\n\n\n\nModule 2:\nDecision Analysis\n\n\n\n\n5\n2/5, 2/7, 2/8\nCost-benefit analysis\nArrow et al. (2013)\n\n\n\n6\n2/12, 2/14, 2/16\nScenario analysis\nBankes (1993)\nFinal Project: Proposal\n\n\n7\n2/19, 2/21, 2/23\nPolicy search and optimization\nBerchum et al. (2019), Schwetschenau et al. (2023)\n\n\n\n8\n2/26, 2/28, 3/1\nMultiobjective policy search\nZarekarizi et al. (2020)\n\n\n\n9\n3/4, 3/6, 3/8\nSequential decision problems\nHerman et al. (2020), Quinn et al. (2017), Fletcher et al. (2019), Sutton & Barto (2018)\n\n\n\n10\n3/18, 3/20, 3/22\nRobustness\nHerman et al. (2015), McPhail et al. (2019)\nRevised Project Proposal; Exam 2\n\n\n\nModule 3:\nThinking Critically\n\n\n\n\n11\n3/25, 3/27, 3/29\nDeep uncertainty\nSchneider (2002), Lempert & Schlesinger (2000), Oreskes et al. (1994)\n\n\n\n12\n4/1, 4/3, 4/5\nEquity and justice\nPollack et al. (2023), Fletcher et al. (2022)\n\n\n\n13\n4/10, 4/12\nFinancial and systemic risks\nThomson et al. (2023), Condon (2021)\n\n\n\n14\n4/15, 4/17, 4/19\nReflections\nKeller et al. (2021), Gilligan & Vandenbergh (2020)\nDraft Project Writeup; Exam 3\n\n\n\n\nFinal Projects\n\n\n\n\n\nTBD\nduring final exam slot\n\nProject presentations\n\n\n\n4/30\n\n\nFinal write-up",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#references",
    "href": "syllabus.html#references",
    "title": "Syllabus",
    "section": "References",
    "text": "References\n\n\nArrow, K., Cropper, M., Gollier, C., Groom, B., Heal, G., Newell, R., et al. (2013). Determining benefits and costs for future generations. Science, 341(6144), 349–350. https://doi.org/10.1126/science.1235665\n\n\nBankes, S. (1993). Exploratory modeling for policy analysis. Operations Research, 41(3), 435–449. https://doi.org/c7rgcr\n\n\nBerchum, E. C. van, Mobley, W., Jonkman, S. N., Timmermans, J. S., Kwakkel, J. H., & Brody, S. D. (2019). Evaluation of flood risk reduction strategies through combinations of interventions. Journal of Flood Risk Management, 12(S2), e12506. https://doi.org/10.1111/jfr3.12506\n\n\nBonnafous, L., Lall, U., & Siegel, J. (2017). A water risk index for portfolio exposure to climatic extremes: Conceptualization and an application to the mining industry. Hydrology and Earth System Sciences, 21(4), 2075–2106. https://doi.org/f96k67\n\n\nCondon, M. (2021). Market myopia’s climate bubble. https://doi.org/10.2139/ssrn.3782675\n\n\nFletcher, S., Lickley, M., & Strzepek, K. (2019). Learning about climate change uncertainty enables flexible water infrastructure planning. Nature Communications, 10(1), 1782. https://doi.org/10.1038/s41467-019-09677-x\n\n\nFletcher, S., Hadjimichael, A., Quinn, J., Osman, K., Giuliani, M., Gold, D., et al. (2022). Equity in Water Resources Planning: A Path Forward for Decision Support Modelers. Journal of Water Resources Planning and Management, 148(7), 02522005. https://doi.org/10.1061/(ASCE)WR.1943-5452.0001573\n\n\nFrank, T. (2022, August 22). Bold New Jersey Shore Flood Rules Could Be Blueprint for Entire U.S. Coast. Scientific American. Retrieved from https://www.scientificamerican.com/article/bold-new-jersey-shore-flood-rules-could-be-blueprint-for-entire-u-s-coast/\n\n\nGilligan, J. M., & Vandenbergh, M. P. (2020). Beyond wickedness: Managing complex systems and climate change. Vanderbilt Law Review, 73, 1777–1810. Retrieved from https://wp0.vanderbilt.edu/lawreview/2020/12/beyond-wickedness-managing-complex-systems-and-climate-change/\n\n\nHerman, J. D., Reed, P. M., Zeff, H. B., & Characklis, G. W. (2015). How should robustness be defined for water systems planning under change? Journal of Water Resources Planning and Management, 141(10), 04015012. https://doi.org/10.1061/(asce)wr.1943-5452.0000509\n\n\nHerman, J. D., Quinn, J. D., Steinschneider, S., Giuliani, M., & Fletcher, S. (2020). Climate adaptation as a control problem: Review and perspectives on dynamic water resources planning under uncertainty. Water Resources Research, e24389. https://doi.org/10.1029/2019wr025502\n\n\nKeller, K., Helgeson, C., & Srikrishnan, V. (2021). Climate risk management. Annual Review of Earth and Planetary Sciences, 49(1), 95–116. https://doi.org/10.1146/annurev-earth-080320-055847\n\n\nLall, U., Johnson, T., Colohan, P., Aghakouchak, A., Arumugam, S., Brown, C., et al. (2018). Chapter 3: Water. In D. R. Reidmiller, D. R. Easterling, K. E. Kunkel, K. L. M. Lewis, T. K. Maycock, & B. C. Stewart, Impacts, Risks, and Adaptation in the United States: The Fourth National Climate Assessment, Volume II. Washington, D.C.: U.S. Global Change Research Program. https://doi.org/10.7930/NCA4.2018.CH3\n\n\nLempert, R. J., & Schlesinger, M. E. (2000). Robust strategies for abating climate change. Climatic Change, 45(3-4), 387–401. https://doi.org/10.1023/A:1005698407365\n\n\nLoucks, D. P. (2017). Water resource systems planning and management: An introduction to methods, models, and applications. Cham: Imprint: Springer.\n\n\nMcPhail, C., Maier, H. R., Kwakkel, J. H., Giuliani, M., Castelletti, A., & Westra, S. (2019). Robustness metrics: How are they calculated, when should they be used and why do they give different results? Earth’s Future, 169–191. https://doi.org/10.1002/2017ef000649\n\n\nOreskes, N., Shrader-Frechette, K., & Belitz, K. (1994). Verification, validation, and confirmation of numerical models in the Earth sciences. Science. https://doi.org/10.1126/science.263.5147.641\n\n\nPollack, A., Helgeson, C., Kousky, C., & Keller, K. (2023, September 15). Transparency on underlying values is needed for useful equity measurements. https://doi.org/10.31219/osf.io/kvyxr\n\n\nQuinn, J. D., Reed, P. M., & Keller, K. (2017). Direct policy search for robust multi-objective management of deeply uncertain socio-ecological tipping points. Environmental Modelling & Software, 92, 125–141. https://doi.org/10.1016/j.envsoft.2017.02.017\n\n\nReed, P. M., Hadjimichael, A., Moss, R. H., Brelsford, C., Burleyson, C. D., Cohen, S., et al. (2022). Multisector Dynamics: Advancing the Science of Complex Adaptive Human-Earth Systems. Earth’s Future, 10(3), e2021EF002621. https://doi.org/10.1029/2021EF002621\n\n\nSchneider, S. H. (2002). Can we estimate the likelihood of climatic changes at 2100? Climatic Change, 52(4), 441–451. https://doi.org/http://dx.doi.org/10.1023/A:1014276210717\n\n\nSchwetschenau, S. E., Kovankaya, Y., Elliott, M. A., Allaire, M., White, K. D., & Lall, U. (2023). Optimizing Scale for Decentralized Wastewater Treatment: A Tool to Address Failing Wastewater Infrastructure in the United States. ACS ES&T Engineering, 3(1), 1–14. https://doi.org/10.1021/acsestengg.2c00188\n\n\nSeneviratne, S. I., Zhang, X., Adnan, M., Badi, W., Dereczynski, C., Di Luca, A., et al. (2021). Weather and climate extreme events in a changing climate. In V. Masson-Delmotte, P. Zhai, A. Pirani, S. L. Connors, C. Péan, S. Berger, et al. (Eds.), Climate change 2021: The physical science basis. Contribution of working group I to the sixth assessment report of the intergovernmental panel on climate change. Book section, Cambridge, UK; New York, NY, USA: Cambridge University Press. https://doi.org/10.1017/9781009157896.013\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An Introduction (Second Edition). Cambridge, Massachusetts; London, England: MIT Press.\n\n\nThomson, H., Zeff, H. B., Kleiman, R., Sebastian, A., & Characklis, G. W. (2023). Systemic Financial Risk Arising From Residential Flood Losses. Earth’s Future, 11(4), e2022EF003206. https://doi.org/10.1029/2022EF003206\n\n\nWing, O. E. J., Pinter, N., Bates, P. D., & Kousky, C. (2020). New insights into US flood vulnerability revealed from flood insurance big data. Nature Communications, 11(1, 1), 1444. https://doi.org/10.1038/s41467-020-15264-2\n\n\nZarekarizi, M., Srikrishnan, V., & Keller, K. (2020). Neglecting uncertainties biases house-elevation decisions to manage riverine flood risks. Nature Communications, 11(1, 1), 5361. https://doi.org/10.1038/s41467-020-19188-9",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "lectures/04-systems.html#references",
    "href": "lectures/04-systems.html#references",
    "title": "Climate Risks to Complex Sytems",
    "section": "References",
    "text": "References\n\n\nAmonkar, Y., Doss-Gollin, J., Farnham, D. J., Modi, V., & Lall, U. (2023). Differential effects of climate change on average and peak demand for heating and cooling across the contiguous USA. Communications Earth & Environment, 4(1, 1), 1–9. https://doi.org/10.1038/s43247-023-01048-1\n\n\nBusby, J. W., Baker, K., Bazilian, M. D., Gilbert, A. Q., Grubert, E., Rai, V., et al. (2021). Cascading risks: Understanding the 2021 winter blackout in Texas. Energy Research & Social Science, 77, 102106. https://doi.org/10.1016/j.erss.2021.102106\n\n\nDoss-Gollin, J., Farnham, D. J., Lall, U., & Modi, V. (2021). How unprecedented was the February 2021 Texas cold snap? Environmental Research Letters. https://doi.org/10.1088/1748-9326/ac0278\n\n\nDoss-Gollin, J., Amonkar, Y., Schmeltzer, K., & Cohan, D. (2023). Improving the representation of climate risks in long-term electricity systems planning: A critical review. Current Sustainable/Renewable Energy Reports. https://doi.org/10.1007/s40518-023-00224-3\n\n\nLee, J., & Dessler, A. E. (2022). The Impact of Neglecting Climate Change and Variability on ERCOT’s Forecasts of Electricity Demand in Texas. Weather, Climate, and Society, 14(2), 499–505. https://doi.org/10.1175/WCAS-D-21-0140.1\n\n\nReed, P. M., Hadjimichael, A., Moss, R. H., Brelsford, C., Burleyson, C. D., Cohen, S., et al. (2022). Multisector Dynamics: Advancing the Science of Complex Adaptive Human-Earth Systems. Earth’s Future, 10(3), e2021EF002621. https://doi.org/10.1029/2021EF002621"
  },
  {
    "objectID": "lectures/01-intro-class-topic.html#references",
    "href": "lectures/01-intro-class-topic.html#references",
    "title": "Welcome to CEVE 421/521!",
    "section": "References",
    "text": "References\n\n\nSatija, N., Collier, K., & Shaw, A. (2016, December 7). Boomtown, flood town. ProPublica: Hell and High Water. Retrieved from https://projects.propublica.org/houston-cypress/"
  },
  {
    "objectID": "lectures/03-vulnerability-exposure-impacts.html#references",
    "href": "lectures/03-vulnerability-exposure-impacts.html#references",
    "title": "Vulnerability, Exposure, and Impacts",
    "section": "Logistics",
    "text": "Logistics\n\nWednesday: discussion questions for Wing et al. (2020) posted\nWednesday after class: troubleshoot computing issues\nFriday: lab\n\n\n\n\n\n\n\n\n\nGidaris, I., Padgett, J. E., Barbosa, A. R., Chen, S., Cox, D., Webb, B., & Cerato, A. (2017). Multiple-Hazard Fragility and Restoration Models of Highway Bridges for Regional Risk and Resilience Assessment in the United States: State-of-the-Art Review. Journal of Structural Engineering, 143(3), 04016188. https://doi.org/10.1061/(ASCE)ST.1943-541X.0001672\n\n\nJongman, B., Ward, P. J., & Aerts, J. C. J. H. (2012). Global exposure to river and coastal flooding: Long term trends and changes. Global Environmental Change, 22(4), 823–835. https://doi.org/10.1016/j.gloenvcha.2012.07.004\n\n\nMoel, H. de, Vliet, M. van, & Aerts, J. C. J. H. (2014). Evaluating the effect of flood damage-reducing measures: A case study of the unembanked area of Rotterdam, the Netherlands. Regional Environmental Change, 14(3), 895–908. https://doi.org/10.1007/s10113-013-0420-z\n\n\nTedesco, M., McAlpine, S., & Porter, J. R. (2020). Exposure of real estate properties to the 2018 Hurricane Florence flooding. Natural Hazards and Earth System Sciences, 20(3), 907–920. https://doi.org/10.5194/nhess-20-907-2020\n\n\nWing, O. E. J., Pinter, N., Bates, P. D., & Kousky, C. (2020). New insights into US flood vulnerability revealed from flood insurance big data. Nature Communications, 11(1, 1), 1444. https://doi.org/10.1038/s41467-020-15264-2"
  },
  {
    "objectID": "lectures/05-bca.html",
    "href": "lectures/05-bca.html",
    "title": "Cost-Benefit Analysis",
    "section": "",
    "text": "We often want a quantitative way to compare two or more decisions. Hence, cost-benefit analysis.\nIt’s a simple idea. For a given “decision” (being deliberately vague about what we mean by this), we need some function that tells us how good or bad the decision is. Then, we can compare the goodness of different decisions.\n\n\n\n\n\n\nAs a motivating example, consider that we are have been asked to help a homeowner decide whether to elevate their home by 5ft to protect against future flooding, or whether to leave it as-is.\n\n\n\n\n\n\n\n\n\nFigure 1: Floodproofing in Houston, TX (Houston Public Media)"
  },
  {
    "objectID": "lectures/05-bca.html#dealing-with-time",
    "href": "lectures/05-bca.html#dealing-with-time",
    "title": "Cost-Benefit Analysis",
    "section": "Dealing with time",
    "text": "Dealing with time\nA key feature of nearly all climate adaptation problems is that costs and benefits are spread out over time. How should we weigh costs and benefits that occur at different times?\n\n\n\n\n\n\nFor example, the up-front cost of elevating a house is a cost that occurs now, while the benefits of reduced flood risk occur in the future.\n\n\n\nThe most common way to deal with this is to use net present value (NPV). The idea is to discount future costs and benefits to the present day. If our discount rate is \\(\\gamma = 2\\% = 0.02\\), then we care about a dollar of benefits in one year the same as we care about 98 cents today. More generally, if we have a discount rate of \\(\\gamma\\) then a dollar of benefits in \\(t\\) years is worth \\((1 - \\gamma) ^ t\\) today.\nThe net present value of a decision is the sum of the present values of all costs and benefits: \\[\nNPV = \\sum_{t=0}^T (1 - \\gamma)^t u(a, \\mathbf{s}_t)\n\\] where \\(T\\) is the time horizon of the decision. Note that we write \\(\\mathbf{s}_t\\) to indicate that the state of the world might have time-dependent variables."
  },
  {
    "objectID": "lectures/05-bca.html#cost-benefit-analysis-in-the-real-world",
    "href": "lectures/05-bca.html#cost-benefit-analysis-in-the-real-world",
    "title": "Cost-Benefit Analysis",
    "section": "Cost-benefit analysis in the real world",
    "text": "Cost-benefit analysis in the real world\nCost-benefit analysis is everywhere! Companies use it to decide whether to invest in new products or technologies, governments use it to decide whether to build new infrastructure or regulate pollution, and much more!\nA standard approach is:\n\nCome up with a state of the world \\(\\mathbf{s}\\) that represents your uncertainties.\nWrite down a utility function \\(u(a, \\mathbf{s})\\) that represents your preferences.\nChoose a discount rate \\(\\gamma\\)\nCalculate the net present value of each decision\nPick the decision with the highest net present value"
  },
  {
    "objectID": "lectures/05-bca.html#critiques-and-limitations",
    "href": "lectures/05-bca.html#critiques-and-limitations",
    "title": "Cost-Benefit Analysis",
    "section": "Critiques and limitations",
    "text": "Critiques and limitations\n\nSimple idea:\n\nAdd up all the costs\nAdd up all the benefits\n\nSometimes it’s hard to combine different things that we care about into a single number!\n\nCost and safety\nImpacts on different groups of people\nWe will revisit this in the context of multi-criteria decision analysis.\n\nIn practice, this often leads us to care only about costs and benefits that are easy to quantify / monetize\n\nValue of ecosystems?\n\nThe limitations of discounting are especially relevant for some types of climate adaptation decisions\n\nWe will revisit this on Wednesday\n\nWe often deal with “deep” uncertainties for which it’s hard to come up with a probability distribution\n\nWe will revisit this much later in the semester\n\nIs Bayesian decision theory a good model of how people actually make decisions?\n\nThis framework was largely formalized by Savage (1954), who postulated that people often behave as though maximizing expected utility\nEllsberg (1961) and others: this is not how people really make decisions!\nOur goal is to support decision-making, not predict how people will actually make decisions. So the fact that this is not how people actually make decisions is not a problem for us.\nThat said: when our predictions of “what is rational” diverge from what people actually do, we should be curious about why instead of assuming they are stupid!"
  },
  {
    "objectID": "lectures/05-bca.html#a-defense",
    "href": "lectures/05-bca.html#a-defense",
    "title": "Cost-Benefit Analysis",
    "section": "A defense",
    "text": "A defense\nCost-benefit analysis is still useful when applied thoughtfully.\n\nIt forces us to be explicit about our assumptions\n\nWhat we care about and how we are measuring it\nWhat we are ignoring\nHow we think about uncertainty\n\nAllows an apples-to-apples comparison of different decisions\n\nUltimately, cost-benefit analysis is a great decision-support tool, but it is not a decision-making tool. When applied well, it’s an iterative process through which we repeatedly refine our understanding of the decision problem. When applied poorly, it’s a black-box process that spits out a number that is used to justify a decision that was already made."
  },
  {
    "objectID": "lectures/05-bca.html#wednesday",
    "href": "lectures/05-bca.html#wednesday",
    "title": "Cost-Benefit Analysis",
    "section": "Wednesday",
    "text": "Wednesday\nDigging a bit deeper:\n\nDiscount rates for problems with long time horizons (Arrow et al., 2013)\nApplications of cost-benefit analysis\n\nSocial cost of carbon\nArmy Corps of Engineers projects\nEPA regulations"
  },
  {
    "objectID": "lectures/05-bca.html#thursday",
    "href": "lectures/05-bca.html#thursday",
    "title": "Cost-Benefit Analysis",
    "section": "Thursday",
    "text": "Thursday\nLab: implementing a simple cost-benefit analysis for house elevation."
  },
  {
    "objectID": "lectures/02-climate-hazard.html#references",
    "href": "lectures/02-climate-hazard.html#references",
    "title": "The Science of Climate Hazard",
    "section": "References",
    "text": "References\n\n\nLall, U., Johnson, T., Colohan, P., Aghakouchak, A., Arumugam, S., Brown, C., et al. (2018). Chapter 3: Water. In D. R. Reidmiller, D. R. Easterling, K. E. Kunkel, K. L. M. Lewis, T. K. Maycock, & B. C. Stewart, Impacts, Risks, and Adaptation in the United States: The Fourth National Climate Assessment, Volume II. Washington, D.C.: U.S. Global Change Research Program. https://doi.org/10.7930/NCA4.2018.CH3\n\n\nSeneviratne, S. I., Zhang, X., Adnan, M., Badi, W., Dereczynski, C., Di Luca, A., et al. (2021). Weather and climate extreme events in a changing climate. In V. Masson-Delmotte, P. Zhai, A. Pirani, S. L. Connors, C. Péan, S. Berger, et al. (Eds.), Climate change 2021: The physical science basis. Contribution of working group I to the sixth assessment report of the intergovernmental panel on climate change. Book section, Cambridge, UK; New York, NY, USA: Cambridge University Press. https://doi.org/10.1017/9781009157896.013"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Helpful Resources",
    "section": "",
    "text": "This page contains links to resources that may be helpful for the course. If you have a resource that you think would be helpful to others, please let me know and I will add it to the list.",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#julia",
    "href": "resources.html#julia",
    "title": "Helpful Resources",
    "section": "Julia",
    "text": "Julia\nThere are lots of great resources on programming and Julia. Here is a curated list of some particularly helpful tools.\n\n\n\n\n\n\nSome of these tutorials provide their own instructions on how to install Julia. Please follow the instructions provided in this course!\n\n\n\n\nGetting Started\n\nJulia for Nervous Begineers: A free course on JuliaAcademy for people who are hesitant but curious about learning to write code in Julia.\nFastTrack to Julia cheatsheet\nComprehensive Julia Tutorials: YouTube playlist covering a variety of Julia topics, starting with an introduciton to the language.\nMatlab-Python-Julia Cheatsheet\n\n\n\nDigging Deeper\n\nIntroduction to Computational Thinking: a great Julia-based course at MIT covering applied mathematics and computational thinking\n\n\n\nPlotting with Makie\n\nMakie Tutorials\nMakieCon 2023 YouTube Channel",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#other-software-tools",
    "href": "resources.html#other-software-tools",
    "title": "Helpful Resources",
    "section": "Other Software Tools",
    "text": "Other Software Tools\n\nGit and GitHub\n\nGit Basics from The Odin Project.\nLearn Git Branching: An interactive, visual tutorial to how git works.\nVersion Control from MIT’s “CS: Your Missing Semester” course.\nGit and GitHub for Poets: YouTube playlist covering the basics of git and GitHub.\nVersion Control with Git course from Software Carpentry\n\n\n\nZotero\n\nZotero and Citation Management by Fondren library\nZotero Quick Start Guide\n\n\n\nTypesetting Math\n\nJustin Bois’ tutorial.\nMarkdown Cheatsheet\nLaTeX Cheatsheet\nMathpix Snip allows you to convert images of equations to LaTeX code (there is a free tier)\nDetexify lets you draw a symbol and suggests the LaTeX code for the corresponding symbol",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#big-picture",
    "href": "resources.html#big-picture",
    "title": "Helpful Resources",
    "section": "Big Picture",
    "text": "Big Picture\n\nUsing Large Language Models (LLMs)\n\nGitHub Copilot is an extension for VS Code that can provide suggestions for code completion and editing. It is free for students and educators.\nBlog: “Bob Carpenter thinks GPT-4 is awesome”: this post highlights how GPT-4 is able to write a program in Stan, a statistical programming language, and also the mistakes that it makes. Finding and correcting these mistakes requires knowing the Stan language and having a deep understanding of the statistical model, but someone with this expertise could potentially use GPT-4 to accelerate their coding workflow. The comments are also interesting and insightful.\nAI Snake Oil is a blog that seeks to dispel hype, remove misconceptions, and clarify the limits of AI. The authors are in the Princeton University Department of Computer Science.\nChatGPT has both free and paid tiers and can be helpful with writing and interpreting code, though care is needed as described above",
    "crumbs": [
      "Resources"
    ]
  }
]